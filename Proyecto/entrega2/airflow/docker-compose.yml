version: '3.8'

# SodAI Drinks Prediction Pipeline - Docker Compose Configuration
# ================================================================
# This docker-compose file orchestrates:
# - Airflow (webserver + scheduler in standalone mode)
# - MLflow UI (for experiment tracking)
# - PostgreSQL (Airflow metadata database)

services:
  # PostgreSQL - Airflow Metadata Database
  postgres:
    image: postgres:15-alpine
    container_name: sodai_postgres
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 10s
      retries: 5
      start_period: 5s
    restart: unless-stopped
    networks:
      - sodai_network

  # Airflow Standalone (Webserver + Scheduler + Triggerer)
  airflow:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: sodai_airflow
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      # Core
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
      - AIRFLOW__CORE__FERNET_KEY=ZmDfcTF7_60GrrY167zsiPd67pEvs0aGOv2oasOM1Pg=
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=False

      # Webserver
      - AIRFLOW__WEBSERVER__EXPOSE_CONFIG=True
      - AIRFLOW__WEBSERVER__SECRET_KEY=super_secret_key_change_in_production

      # Scheduler
      - AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL=30

      # MLflow
      - MLFLOW_TRACKING_URI=http://mlflow:5000

      # Admin user (created on first run)
      - _AIRFLOW_WWW_USER_CREATE=True
      - _AIRFLOW_WWW_USER_USERNAME=admin
      - _AIRFLOW_WWW_USER_PASSWORD=admin

      # Python
      - PYTHONPATH=/opt/airflow/dags
    volumes:
      # DAGs and modules (read-write for hot reload)
      - ./dags:/opt/airflow/dags

      # Data (persistent)
      - ./data:/opt/airflow/data

      # MLflow runs (persistent)
      - mlflow_data:/opt/airflow/mlruns

      # Models (persistent)
      - ./models:/opt/airflow/models

      # Predictions (persistent)
      - ./predictions:/opt/airflow/predictions

      # Drift reports (persistent)
      - ./drift_reports:/opt/airflow/drift_reports

      # Logs (persistent)
      - airflow_logs:/opt/airflow/logs

      # Utility scripts
      - ./generate_test_data.py:/opt/airflow/generate_test_data.py
    ports:
      - "8080:8080"  # Airflow UI
    command: >
      bash -c "
        airflow db migrate &&
        airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com || true &&
        airflow standalone
      "
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    restart: unless-stopped
    networks:
      - sodai_network

  # MLflow UI Server
  mlflow:
    image: ghcr.io/mlflow/mlflow:v2.17.2
    container_name: sodai_mlflow
    ports:
      - "5000:5000"
    volumes:
      - mlflow_data:/mlflow
    command: >
      mlflow server
      --backend-store-uri file:///mlflow
      --default-artifact-root file:///mlflow/artifacts
      --host 0.0.0.0
      --port 5000
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:5000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    restart: unless-stopped
    networks:
      - sodai_network

# Volumes for data persistence
volumes:
  postgres_data:
    name: sodai_postgres_data
  mlflow_data:
    name: sodai_mlflow_data
  airflow_logs:
    name: sodai_airflow_logs

# Network
networks:
  sodai_network:
    name: sodai_network
    driver: bridge
