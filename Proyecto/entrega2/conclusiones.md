# Conclusiones - Entrega 2: Pipeline MLOps para Predicción de Compras

## 1. Impacto de las herramientas de tracking y despliegue en el desarrollo

La incorporación de MLflow, Airflow y Docker transformó significativamente la forma en que abordamos el desarrollo del proyecto. En entregas anteriores, el versionado de experimentos era manual y propenso a errores, con archivos dispersos y documentación inconsistente de hiperparámetros. La implementación de MLflow resolvió este problema al proporcionar un registro automático y sistemático de cada experimento, incluyendo hiperparámetros, métricas, artefactos y metadatos del entorno de ejecución.

La integración entre MLflow y Optuna resultó particularmente valiosa. Cada uno de los 50 trials de optimización de hiperparámetros se registró automáticamente como nested runs dentro de MLflow, permitiendo analizar retrospectivamente qué combinaciones de parámetros funcionaron mejor. Este análisis reveló que `max_depth` y `learning_rate` tienen el mayor impacto en el desempeño del modelo, mientras que parámetros como `gamma` contribuyen marginalmente. Esta información no solo mejoró nuestra comprensión del modelo, sino que también nos permitió reducir el espacio de búsqueda en iteraciones posteriores.

Airflow aportó robustez mediante la automatización completa del pipeline. La curva de aprendizaje fue considerable, especialmente al trabajar con conceptos como XCom para compartir estado entre tareas y BranchPythonOperator para implementar lógica condicional. Sin embargo, una vez operativo, el sistema demostró su valor al ejecutarse de forma autónoma: procesa nuevos datos, detecta drift estadístico y decide automáticamente si es necesario reentrenar el modelo. La interfaz gráfica de Airflow facilitó enormemente el debugging al mostrar claramente el estado de cada tarea y permitir re-ejecuciones parciales del pipeline.

Docker resolvió el problema de reproducibilidad del entorno de ejecución. La containerización garantizó que todos los miembros del equipo trabajaran con versiones idénticas de dependencias, eliminando los clásicos problemas de "funciona en mi máquina". Más importante aún, permitió el aislamiento de servicios: Airflow, MLflow, backend FastAPI y frontend Gradio corren en contenedores independientes que se comunican a través de redes Docker, lo que facilita el escalamiento y el mantenimiento de cada componente por separado.

## 2. Desafíos del despliegue con FastAPI y Gradio

El despliegue de la aplicación web presentó varios desafíos técnicos que requirieron soluciones pragmáticas. El más crítico fue la gestión de memoria durante el entrenamiento del modelo. Los primeros intentos de ejecutar Optuna con 50 trials sobre el dataset completo (aproximadamente 1.5M filas) resultaron en errores de memoria que colapsaban los contenedores Docker al superar los 8GB de RAM disponibles. Esto llevó a implementar una estrategia de sampling estratégico: durante la optimización de hiperparámetros se utiliza 20% del conjunto de entrenamiento y 30% del conjunto de validación, permitiendo que Optuna explore el espacio de búsqueda sin agotar los recursos. Una vez identificados los mejores hiperparámetros, el modelo final se entrena con el 100% de los datos. Esta solución representa un compromiso razonable entre eficiencia computacional y desempeño del modelo.

La integración entre el backend FastAPI y el pipeline de Airflow presentó desafíos de sincronización. El backend carga el modelo al inicializarse, pero si Airflow entrena un modelo nuevo durante la ejecución del backend, este último no lo detecta automáticamente. Se implementó un endpoint `/reload-model` que permite actualizar el modelo sin reiniciar el contenedor completo, junto con verificación de timestamps para detectar cuando hay modelos más recientes disponibles. Además, se agregó una estrategia de fallback robusta: el sistema intenta cargar el modelo primero desde MLflow, luego desde archivo local, y finalmente devuelve un mensaje de error informativo si ningún modelo está disponible.

El problema más complejo fue garantizar consistencia en el feature engineering entre entrenamiento y predicción. Las features temporales como recency, frequency y trend se calculan usando operaciones de ventana deslizante (`.shift()` y `.rolling()`), que deben replicarse exactamente durante la inferencia. Inicialmente, el modelo entrenaba correctamente pero generaba predicciones inconsistentes debido a diferencias en cómo se calculaban estas features. La solución fue encapsular toda la lógica de feature engineering en clases de sklearn custom (`FeatureEngineer`, `GeoClusterer`) que se serializan como parte del pipeline completo. Esto garantiza que el estado del preprocesamiento se preserva y se aplica idénticamente tanto en entrenamiento como en predicción.

Respecto a Gradio, resultó una herramienta adecuada para el prototipado rápido de interfaces, permitiendo crear una aplicación funcional en pocas líneas de código. Sin embargo, presenta limitaciones para aplicaciones de producción más complejas: la personalización de diseño es restringida, la lógica de negocio tiende a mezclarse con la capa de presentación, y las consideraciones de seguridad y autenticación requieren configuración adicional. Para un proyecto académico cumple su propósito, pero en un escenario productivo real sería preferible utilizar un framework frontend dedicado como React o Vue comunicándose con el backend FastAPI a través de su API REST.

## 3. Contribución de Airflow a la robustez y escalabilidad del pipeline

Airflow demostró ser fundamental para la robustez y escalabilidad del sistema de ML en producción. Su principal aporte es la capacidad de orquestar flujos de trabajo complejos con dependencias claras entre tareas, permitiendo que el pipeline se ejecute de forma completamente automatizada y programada. La implementación de branching condicional mediante `BranchPythonOperator` permitió crear un sistema adaptativo que decide inteligentemente si reentrenar el modelo basándose en la detección de drift estadístico, evitando reentrenamientos innecesarios que consumirían recursos computacionales sin beneficio real.

La interfaz gráfica de Airflow proporciona visibilidad completa del estado del pipeline, mostrando qué tareas están en ejecución, cuáles fallaron y cuáles completaron exitosamente. Esta transparencia facilitó enormemente el debugging durante el desarrollo, permitiendo identificar rápidamente qué componente del pipeline presentaba problemas. La capacidad de re-ejecutar tareas individuales o grupos de tareas sin necesidad de correr todo el pipeline desde cero resultó invaluable para iterar rápidamente durante el desarrollo.

Desde la perspectiva de escalabilidad, Airflow permite ejecutar tareas en paralelo cuando no tienen dependencias entre sí, distribuyendo la carga de trabajo eficientemente. Aunque en este proyecto las tareas son mayormente secuenciales debido a sus dependencias de datos, la arquitectura está preparada para escalar horizontalmente mediante la adición de workers adicionales. Esto sería especialmente útil si en el futuro se quisiera entrenar múltiples modelos simultáneamente (por ejemplo, modelos específicos por región geográfica).

Un aspecto importante es la gestión de errores y reintentos. Airflow permite configurar reintentos automáticos de tareas que fallan (por ejemplo, debido a problemas transitorios de red o recursos temporalmente no disponibles), así como configurar alertas cuando las tareas fallan definitivamente. Esta robustez es crítica para un sistema que se espera funcione de forma autónoma en producción sin intervención manual constante.

## 4. Mejoras propuestas para versiones futuras

El sistema actual sienta bases sólidas para un pipeline MLOps productivo, pero existen varias áreas donde mejoras adicionales incrementarían significativamente su valor en un escenario real.

### Automatización y monitoreo avanzado

Actualmente el sistema detecta drift en las distribuciones de datos, pero no monitorea la degradación del desempeño del modelo en producción. Una mejora crítica sería implementar un sistema de monitoreo que compare las predicciones del modelo contra las ventas reales que ocurren la semana siguiente. Esto permitiría detectar "performance drift": situaciones donde las distribuciones de datos se mantienen similares pero el modelo deja de predecir correctamente debido a cambios en las dinámicas del negocio. La integración con herramientas como Prometheus y Grafana permitiría visualizar métricas del modelo en tiempo real y configurar alertas automáticas cuando métricas clave caen por debajo de umbrales aceptables.

El logging actualmente es básico y se basa en prints a stdout. Implementar logging estructurado en formato JSON facilitaría enormemente el análisis post-mortem de errores y la identificación de patrones en fallos del sistema. Esto sería especialmente valioso al escalar el sistema, donde rastrear problemas manualmente se vuelve impracticable.

### Optimización de rendimiento y escalabilidad

El procesamiento actual de predicciones, aunque funcional, no está optimizado para datasets masivos. La generación de predicciones para todos los pares cliente-producto requiere crear un universo de aproximadamente 1.5 millones de filas, lo que consume recursos considerables. Implementar caché de features pre-calculadas que se actualicen incrementalmente (en lugar de recalcularse completamente cada vez) aceleraría significativamente este proceso.

La arquitectura actual usa archivos Parquet para almacenar datos, lo que es adecuado para este volumen pero limitaría la escalabilidad. Migrar a una base de datos relacional como PostgreSQL permitiría queries más eficientes, especialmente para consultas interactivas desde la interfaz web. Además, implementar una cola de mensajes (RabbitMQ o Redis) para procesar predicciones de forma asíncrona mejoraría la experiencia del usuario al no tener que esperar a que se generen todas las predicciones antes de recibir respuesta.

### Enriquecimiento del modelo

El modelo actual utiliza principalmente features basadas en comportamiento histórico de compra, pero ignora información contextual valiosa como precios, promociones, stock disponible y estacionalidad. Incorporar estas variables podría mejorar significativamente la precisión de las predicciones. Por ejemplo, un producto puede tener alta probabilidad de compra basado en comportamiento histórico, pero si está fuera de stock la predicción sería incorrecta.

La estacionalidad es particularmente relevante en el negocio de bebidas, donde ciertos productos tienen mayor demanda en fechas específicas (bebidas calientes en invierno, refrescos en verano, productos específicos en festividades). Agregar features de calendario (día de la semana, mes, festivos) capturaría estos patrones temporales.

### Validación y testing

El sistema actual carece de tests automatizados, lo que introduce riesgo al hacer cambios en el código. Implementar una suite de tests unitarios y de integración que se ejecute automáticamente en cada cambio (idealmente mediante CI/CD con GitHub Actions o similar) aumentaría significativamente la confiabilidad del sistema. Esto es especialmente importante en un sistema de ML donde pequeños cambios en el código de preprocesamiento pueden tener efectos sutiles pero significativos en las predicciones.

## 5. Reflexiones finales

La implementación de este sistema MLOps completo reveló que el desarrollo de modelos de ML para producción difiere sustancialmente del modelado experimental en notebooks. La distribución del esfuerzo fue aproximadamente 30% en modelado (selección de algoritmo, feature engineering, optimización de hiperparámetros) y 70% en ingeniería del sistema (integración de componentes, gestión de datos, deployment, testing). Esta proporción contrasta marcadamente con proyectos académicos tradicionales donde el enfoque principal es el modelo mismo.

La modularización del código resultó esencial no solo para el mantenimiento sino también para el debugging. Separar claramente las responsabilidades (preprocesamiento, feature engineering, entrenamiento, predicción, detección de drift) en módulos independientes facilitó identificar y corregir problemas sin afectar otros componentes del sistema. Esta arquitectura también facilita la evolución futura del sistema, permitiendo reemplazar o mejorar componentes individuales sin necesidad de reescribir todo el pipeline.

La inversión de tiempo en automatización, aunque significativa al inicio, se justifica ampliamente al eliminar trabajo manual repetitivo y reducir la probabilidad de errores humanos. Un sistema que se ejecuta automáticamente, detecta problemas, y se adapta a nuevos datos sin intervención humana es fundamental para que un modelo de ML pueda operar de forma sostenible en producción. Sin Airflow, alguien tendría que acordarse de ejecutar manualmente el pipeline cada semana, verificar que no haya errores, y decidir si es necesario reentrenar el modelo, una carga operativa insostenible a largo plazo.

Finalmente, este proyecto evidenció la importancia de diseñar con visión de producción desde el inicio. Decisiones arquitectónicas tempranas, como usar pipelines de sklearn para garantizar consistencia entre entrenamiento y predicción, o implementar fallback strategies para cargar modelos, evitaron costosas refactorizaciones posteriores. El sistema resultante, aunque mejorable, proporciona una base sólida que podría evolucionar hacia un sistema productivo real con las adiciones y refinamientos descritos en la sección anterior.
