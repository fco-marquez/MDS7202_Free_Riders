{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kl0JoW4Eodvi"
   },
   "source": [
    "# **Laboratorio 9: Airflow **\n",
    "\n",
    "<center><strong>MDS7202: Laboratorio de Programaci贸n Cient铆fica para Ciencia de Datos - Primavera 2025</strong></center>\n",
    "\n",
    "### Cuerpo Docente:\n",
    "\n",
    "- Profesores: Diego Cortez, Gabriel Iturra\n",
    "- Auxiliares: Melanie Pe帽a, Valentina Rojas\n",
    "- Ayudantes: Nicol谩s Cabello, Cristopher Urbina"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o3ypG7Fsodvj"
   },
   "source": [
    "### Equipo: SUPER IMPORTANTE - notebooks sin nombre no ser谩n revisados\n",
    "\n",
    "- Francisco M谩rquez\n",
    "- Santiago Haberle\n",
    "\n",
    "### **Link de repositorio de GitHub:** [Insertar Repositorio](https://github.com/fco-marquez/MDS7202_Free_Riders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J_P7PCPTodvk"
   },
   "source": [
    "## Temas a tratar\n",
    "\n",
    "- Construcci贸n de pipelines productivos usando `Airflow`.\n",
    "\n",
    "## Reglas:\n",
    "\n",
    "- **Grupos de 2 personas**\n",
    "- Fecha de entrega: Entregas Martes a las 23:59.\n",
    "- Instrucciones del lab el viernes a las 16:15 en formato online. Asistencia no es obligatoria, pero se recomienda **fuertemente** asistir.\n",
    "- <u>Prohibidas las copias</u>. Cualquier intento de copia ser谩 debidamente penalizado con el reglamento de la escuela.\n",
    "- Tienen que subir el laboratorio a u-cursos y a su repositorio de github. Labs que no est茅n en u-cursos no ser谩n revisados. Recuerden que el repositorio tambi茅n tiene nota.\n",
    "- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente ser谩n respondidos por este medio.\n",
    "- Pueden usar cualquier material del curso que estimen conveniente.\n",
    "\n",
    "### Objetivos principales del laboratorio\n",
    "\n",
    "- Reconocer los componentes pricipales de `Airflow` y su funcionamiento.\n",
    "- Poner en pr谩ctica la construcci贸n de pipelines de `Airflow`.\n",
    "- Automatizar procesos t铆picos de un proyecto de ciencia de datos mediante `Airflow` y `Docker`.\n",
    "\n",
    "El laboratorio deber谩 ser desarrollado sin el uso indiscriminado de iteradores nativos de python (aka \"for\", \"while\"). La idea es que aprendan a exprimir al m谩ximo las funciones optimizadas que nos entrega `pandas`, las cuales vale mencionar, son bastante m谩s eficientes que los iteradores nativos sobre DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KsfK981Uodvk"
   },
   "source": [
    "# **Introducci贸n**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ilM8YDjodvk"
   },
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"https://media.tenor.com/OBQ6niqbxswAAAAM/legallyblonde.gif\" width=\"300\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7zrLPQNBodvk"
   },
   "source": [
    "Vale, una estudiante del Mag铆ster en Ciencia de Datos, se encuentra en la etapa final de sus estudios. Por un lado, est谩 muy contenta por haber llegado tan lejos, pero por otro, no puede evitar sentirse inquieta. Desde que ingres贸 a la universidad, una pregunta la ha perseguido: 驴qu茅 tan probable es que pueda ser seleccionada en los lugares donde env铆e postulaciones para puestos de trabajo?\n",
    "\n",
    "Esta duda la mantiene en constante reflexi贸n, especialmente porque sabe que el mercado laboral en Ciencia de Datos es competitivo y exige habilidades no solo t茅cnicas, sino tambi茅n estrat茅gicas para destacar. Sin embargo, Vale actualmente est谩 completamente enfocada en terminar su tesis de mag铆ster y ha tenido que postergar cualquier preparaci贸n espec铆fica para enfrentar el desaf铆o de las postulaciones laborales.\n",
    "\n",
    "Al ver el avance y las habilidades que usted ha demostrado en el curso, Vale decidi贸 proponerle un desaf铆o que le permitir谩 disminuir la incertidumbre sobre su futuro laboral. Inspirado en sus conocimientos, recolect贸 un conjunto de datos que contiene informaci贸n sobre diversos factores que influyen en las decisiones de contrataci贸n de empresas al seleccionar entre sus postulantes. Este set de datos incluye los siguientes atributos:\n",
    "\n",
    "- Age: Edad del candidato\n",
    "- Gender: Genero del candidato. Male (0), Female (1).\n",
    "- EducationLevel: Mayor nivel educacional alcanzado por el candidato. Licenciatura Tipo 1 (1), Licenciatura Tipo 2 (2), Maestr铆a (3), PhD. (4).\n",
    "- ExperienceYears: A帽os de experiencia profesional.\n",
    "- PreviousCompanies: Numero de compa帽铆as donde el candidato ha trabajado anteriormente.\n",
    "- DistanceFromCompany: Distancia en kilometros entre la residencia del candidato y la compa帽铆a donde postula.\n",
    "- InterviewScore: Puntaje obtenido en la entrevista por el candidato entre 0 a 100.\n",
    "- SkillScore: Puntaje obtenido en evaluaci贸n de habilidades t茅cnicas por el candidato, entre 0 a 100.\n",
    "- PersonalityScore: Puntaje obtenido en pruebas de personalidad del candidato, entre 0 a 100.\n",
    "- RecruitmentStrategy: Estrategia del equipo de reclutamiento. Agresiva (1), Moderada (2), Conservadora (3).\n",
    "\n",
    "Variable a predecir:\n",
    "- HiringDecision: Resultado de la postulaci贸n. No contratado (0), Contratado (1).\n",
    "\n",
    "Su objetivo ser谩 ayudar a Vale a desarrollar un modelo que le permita predecir, basado en estos factores, si un postulante ser谩 contratado o no. Esta herramienta no solo le dar谩 a Vale mayor claridad sobre el impacto de ciertos atributos en la decisi贸n final de contrataci贸n, sino que tambi茅n le permitir谩 aplicar sus conocimientos de Ciencia de Datos para resolver una pregunta que a muchos estudiantes como a ella les inquieta.\n",
    "\n",
    "Como estudiante del curso Laboratorio de Programaci贸n Cient铆fica para Ciencia de Datos, deber谩 demostrar sus capacidades para preprocesar, analizar y modelar datos, brind谩ndole a Vale una soluci贸n robusta y bien fundamentada para su problem谩tica.\n",
    "\n",
    "`Nota:` El siguiente [enlace](https://www.kaggle.com/datasets/rabieelkharoua/predicting-hiring-decisions-in-recruitment-data/data) contiene el set de datos original."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yeh268atodvl"
   },
   "source": [
    "# **1. Pipeline de Predicci贸n Lineal** (30 Puntos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CmB1LTWnodvl"
   },
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"https://media.licdn.com/dms/image/v2/D4E22AQHZplrdPyKnvA/feedshare-shrink_2048_1536/feedshare-shrink_2048_1536/0/1713736729086?e=2147483647&v=beta&t=Tad2ulaWkhhDrPRN0PCdXrfuza60PjoJqgLborDyLao\" width=\"500\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bF1bTY0Modvl"
   },
   "source": [
    "En esta secci贸n buscaremos desplegar un producto utilizando un modelo de clasificaci贸n `Random Forest` para determinar **si una persona ser谩 contratada o no en un proceso de selecci贸n**. Para ello, comenzaremos preparando un pipeline lineal mediante `Airflow`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7MllF4fodvl"
   },
   "source": [
    "## **1.1 Preparando el Pipeline** (15 puntos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V1JxaZgModvl"
   },
   "source": [
    "**Primero, aseg煤rese de tener creada las carpetas `dags`, `plugins` y `logs`**.\n",
    "\n",
    "Comenzamos preparando un archivo llamado `hiring_functions.py`, el cual guardar谩 en la carpeta `dags` y debe contener lo siguiente:\n",
    "\n",
    "1. (3 puntos) Una funci贸n llamada `create_folders()` que cree una carpeta, la cual utilice la fecha de ejecuci贸n como nombre. Adicionalmente, dentro de esta carpeta debe crear las siguientes subcarpetas:\n",
    "  - raw\n",
    "  - splits\n",
    "  - models\n",
    "\n",
    "  `Hint`: Puede hacer uso de kwargs para obtener la fecha de ejecuci贸n mediante el DAG. El siguiente [Enlace](https://airflow.apache.org/docs/apache-airflow/stable/templates-ref.html) le puede ser 煤til.\n",
    "\n",
    "2. (3 puntos) Una funci贸n llamada `split_data()` que lea el archivo `data_1.csv` de la carepta `raw` y a partir de este, aplique un *hold out*, generando un dataset de entrenamiento y uno de prueba. Luego debe guardar estos nuevos conjuntos de datos en la carpeta `splits`. `Nota:` Utilice un 20% para el conjunto de prueba, mantenga la proporci贸n original en la variable objetivo y fije una semilla.\n",
    "\n",
    "3. (8 puntos) Cree una funci贸n llamada `preprocess_and_train()` que:\n",
    "  - Lea los set de entrenamiento y prueba de la carpeta `splits`.\n",
    "  - Cree y aplique un `Pipeline` con una etapa de preprocesamiento. Utilice `ColumnTransformers` para aplicar las transformaciones que estime convenientes. Puede apoyarse del archivo `data_1_report.html` para justificar cualquier paso del preprocesamiento.\n",
    "  \n",
    "  - A帽ada una etapa de entrenamiento utilizando el modelo `RandomForest`.\n",
    "  \n",
    "  Esta funci贸n **debe crear un archivo `joblib` (an谩logo a `pickle`) con el pipeline entrenado** en la carepta `models`, adem谩s debe **imprimir** el accuracy en el conjunto de prueba y el f1-score de la clase positiva (contratado).\n",
    "3. (1 punto) Incorpore la funci贸n `gradio_interface` en su script, modificando la ruta de acceso a su modelo, de forma que pueda leerlo desde la carepta `models`. Puede realizar las modificaciones que estime necesarias.\n",
    "\n",
    "`NOTA:` Se permite la creaci贸n de funciones auxiliares si lo estiman conveniente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ze9Iotloodvl"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import gradio as gr\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from airflow.models import Variable\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "\n",
    "\n",
    "def create_folders(ds, **kwargs):\n",
    "    base_path = f\"/root/airflow/{ds}\"\n",
    "    os.makedirs(f\"{base_path}/raw\", exist_ok=True)\n",
    "    os.makedirs(f\"{base_path}/splits\", exist_ok=True)\n",
    "    os.makedirs(f\"{base_path}/models\", exist_ok=True)\n",
    "\n",
    "\n",
    "def split_data(ds, **kwargs):\n",
    "    base_path = f\"/root/airflow/{ds}\"\n",
    "    # Leer archivo data_1.csv desde la carpeta raw\n",
    "    df = pd.read_csv(f\"{base_path}/raw/data_1.csv\")\n",
    "    # Dividir los datos en conjuntos de entrenamiento y prueba (80% - 20%)\n",
    "    train_df, test_df = train_test_split(\n",
    "        df, test_size=0.2, stratify=df[\"HiringDecision\"], random_state=1892\n",
    "    )\n",
    "    # Guardar los conjuntos divididos en la carpeta splits\n",
    "    train_df.to_csv(f\"{base_path}/splits/train.csv\", index=False)\n",
    "    test_df.to_csv(f\"{base_path}/splits/test.csv\", index=False)\n",
    "\n",
    "\n",
    "def preprocess_and_train(ds, **kwargs):\n",
    "    base_path = f\"/root/airflow/{ds}\"\n",
    "    # Leer los conjuntos de entrenamiento y prueba desde la carpeta splits\n",
    "    train_df = pd.read_csv(f\"{base_path}/splits/train.csv\")\n",
    "    test_df = pd.read_csv(f\"{base_path}/splits/test.csv\")\n",
    "\n",
    "    # Separar caracter铆sticas y etiquetas\n",
    "    X_train = train_df.drop(\"HiringDecision\", axis=1)\n",
    "    y_train = train_df[\"HiringDecision\"]\n",
    "    X_test = test_df.drop(\"HiringDecision\", axis=1)\n",
    "    y_test = test_df[\"HiringDecision\"]\n",
    "\n",
    "    # Pipeline de preprocesamiento\n",
    "    numeric_features = [\n",
    "        \"Age\",\n",
    "        \"DistanceFromCompany\",\n",
    "        \"InterviewScore\",\n",
    "        \"SkillScore\",\n",
    "        \"PersonalityScore\",\n",
    "    ]\n",
    "    ordinal_features = [\"EducationLevel\", \"ExperienceYears\"]\n",
    "\n",
    "    categorical_features = [\"Gender\", \"RecruitmentStrategy\", \"PreviousCompanies\"]\n",
    "\n",
    "    # Pipeline de preprocesamiento\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", StandardScaler(), numeric_features),\n",
    "            (\"ord\", OrdinalEncoder(), ordinal_features),\n",
    "            (\n",
    "                \"cat\",\n",
    "                OneHotEncoder(drop=\"first\", sparse_output=False),\n",
    "                categorical_features,\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Pipeline completo: preprocesamiento + modelo\n",
    "    pipeline = Pipeline(\n",
    "        [\n",
    "            (\"preprocessor\", preprocessor),\n",
    "            (\"classifier\", RandomForestClassifier(random_state=1892, n_estimators=100)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Entrenar el pipeline completo\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Realizar predicciones en el conjunto de prueba\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "    # Calcular m茅tricas\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, pos_label=1)  # Clase positiva = 1 (Contratado)\n",
    "\n",
    "    print(f\"Model Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1-Score (clase positiva - Contratado): {f1:.4f}\")\n",
    "\n",
    "    # Guardar el pipeline entrenado como archivo joblib\n",
    "    model_path = f\"/root/airflow/{ds}/models/hiring_model.joblib\"\n",
    "    joblib.dump(pipeline, model_path)\n",
    "    print(f\"Modelo guardado en: {model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K_RCVPnUodvm"
   },
   "outputs": [],
   "source": [
    "def predict(file, model_path):\n",
    "\n",
    "    pipeline = joblib.load(model_path)\n",
    "    input_data = pd.read_json(file)\n",
    "    predictions = pipeline.predict(input_data)\n",
    "    print(f\"La prediccion es: {predictions}\")\n",
    "    labels = [\"No contratado\" if pred == 0 else \"Contratado\" for pred in predictions]\n",
    "\n",
    "    return {\"Predicci贸n\": labels[0]}\n",
    "\n",
    "\n",
    "def gradio_interface(ds, **kwargs):\n",
    "\n",
    "    model_path = f\"/root/airflow/{ds}/models/hiring_model.joblib\"\n",
    "\n",
    "    interface = gr.Interface(\n",
    "        fn=lambda file: predict(file, model_path),\n",
    "        inputs=gr.File(label=\"Sube un archivo JSON\"),\n",
    "        outputs=\"json\",\n",
    "        title=\"Hiring Decision Prediction\",\n",
    "        description=\"Sube un archivo JSON con las caracter铆sticas de entrada para predecir si Vale ser谩 contratada o no.\",\n",
    "    )\n",
    "    interface.launch(share=True, server_name=\"0.0.0.0\", server_port=7860)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lTKOj1hfodvm"
   },
   "source": [
    "## **1.2 Creando Nuestro DAG** (15 puntos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RkEZcEh4odvm"
   },
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"https://media.tenor.com/a_yibuZQgngAAAAM/elle-woods.gif\" width=\"400\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r-MTaxTgodvm"
   },
   "source": [
    "Con las funciones del pipeline ya creadas, ahora vamos a proceder a crear un Directed Acyclic Graph (DAG). Para ello, se le pide lo siguiente:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y-yUak2Rodvm"
   },
   "source": [
    "- (10 puntos) Cree un segundo archivo llamado `dag_lineal.py` y guardelo en la carpeta dags. Este script debe seguir la siguiente estructura (Ver imagen de referencia):\n",
    "\n",
    "    0. Inicialice un DAG con fecha de inicio el 1 de octubre de 2024, ejecuci贸n manual y **sin backfill**. Asigne un `dag_id` que pueda reconocer facilmente, como `hiring_lineal`, etc.\n",
    "    1. Debe comenzar con un marcador de posici贸n que indique el inicio del pipeline.\n",
    "    2. Cree una carpeta correspondiente a la ejecuci贸n del pipeline y cree las subcarpetas `raw`, `splits` y `models` mediante la funci贸n `create_folders()`.\n",
    "    3. Debe descargar el archivo `data_1.csv` del siguiente [enlace](https://gitlab.com/eduardomoyab/laboratorio-13/-/raw/main/files/data_1.csv). Debe guardar el archivo en la carpeta raw de la ejecuci贸n correspondiente.`Hint:` Le puede ser 煤til el comando `curl -o <path de guardado> <enlace con los datos>`.\n",
    "    4. Debe aplicar un hold out mediante la funci贸n `split_data()` de su archivo creado en la subsecci贸n anterior.\n",
    "    5. Debe aplicar el preprocesamiento y el entrenamiento del modelo mediante la funci贸n `preprocess_and_train()`.\n",
    "    6. Finalmente, debe montar una interfaz en gradio donde pueda cargar un archivo ``json``.\n",
    "\n",
    "\n",
    "- (3 puntos) Cree un `DockerFile` para montar un contenedor que contenga Airflow. Adicionalmente, cree una carpeta llamada dags donde guardar谩 el script.py creado anteriormente.\n",
    "\n",
    "    `Nota:` Para la imagen, se recomienda utilizar python 3.10-slim. Adicionalmente, puede instalar `curl` mediante la siguiente linea de c贸digo: `RUN apt-get update && apt-get install -y curl`.\n",
    "\n",
    "- Construya el contenedor en Docker y acceda a la aplicaci贸n web de Airflow mediante el siguiente [enlace](http://localhost:8080/). Inicie sesi贸n, acceda al DAG creado y ejecute de forma manual su pipeline.\n",
    "\n",
    "- (2 puntos) Acceda a la URL p煤blica de Gradio e ingrese el archivo `vale_data.json` a su modelo. 驴Que predicci贸n entreg贸 el modelo para Vale? Adjunte im谩genes de su resultado. `Hint:` Puede acceder a los `logs` para obtener los prints y la URL p煤blica.\n",
    "\n",
    "`Hint:` Recuerde que puede entregar `kwargs` a sus funciones, como por ejemplo la fecha de ejecuci贸n `ds`.\n",
    "\n",
    "**Para esta secci贸n, debe adjuntar todos los scripts creados junto a su notebook en la entrega, ya que ser谩n ejecutados para validar el funcionamiento. Para justificar sus respuestas, adicionaslmente puede utilizar im谩genes de apoyo, como screenshots.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tiMTgQfJpuIv"
   },
   "source": [
    "DAG de referencia:\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://drive.google.com/uc?id=1iwDgECZfFeWq1dl433tMa6_3CNF9cn1L\" width=\"1200\">\n",
    "</p>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ckzDqsF4odvn"
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.operators.empty import EmptyOperator\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.utils.dates import days_ago\n",
    "from hiring_functions import (\n",
    "    create_folders,\n",
    "    gradio_interface,\n",
    "    predict,\n",
    "    preprocess_and_train,\n",
    "    split_data,\n",
    ")\n",
    "\n",
    "default_args = {\n",
    "    \"owner\": \"free-riders\",\n",
    "    \"retries\": 1,\n",
    "}\n",
    "\n",
    "with DAG(\n",
    "    dag_id=\"hiring_lineal\",\n",
    "    default_args=default_args,\n",
    "    description=\"Hiring Decision Workflow with Linear DAG\",\n",
    "    start_date=dt.datetime(2024, 10, 1),\n",
    "    schedule_interval=None,\n",
    "    catchup=False,\n",
    ") as dag:\n",
    "\n",
    "    # Task 1 - Just a simple print statement\n",
    "    dummy_task = EmptyOperator(task_id=\"Starting_the_process\", retries=2)\n",
    "\n",
    "    # Task 2 - Create necessary folders\n",
    "    create_folders_task = PythonOperator(\n",
    "        task_id=\"create_folders\",\n",
    "        python_callable=create_folders,\n",
    "        op_kwargs={\"ds\": \"{{ ds }}\"},\n",
    "    )\n",
    "\n",
    "    # Task 3 - Download data using BashOperator\n",
    "    task_download_dataset_1 = BashOperator(\n",
    "        task_id=\"download_dataset_1\",\n",
    "        bash_command=\"curl -o \"\n",
    "        \"/root/airflow/{{ ds }}/raw/data_1.csv \"\n",
    "        \"https://gitlab.com/eduardomoyab/laboratorio-13/-/raw/main/files/data_1.csv\",\n",
    "    )\n",
    "\n",
    "    # Task 4 - Split data\n",
    "    split_data_task = PythonOperator(\n",
    "        task_id=\"split_data\", python_callable=split_data, op_kwargs={\"ds\": \"{{ ds }}\"}\n",
    "    )\n",
    "\n",
    "    # Task 5 - Preprocess and train model\n",
    "    preprocess_and_train_task = PythonOperator(\n",
    "        task_id=\"preprocess_and_train\",\n",
    "        python_callable=preprocess_and_train,\n",
    "        op_kwargs={\"ds\": \"{{ ds }}\"},\n",
    "    )\n",
    "\n",
    "    # Task 6 - Set up Gradio interface\n",
    "    gradio_interface_task = PythonOperator(\n",
    "        task_id=\"gradio_interface\",\n",
    "        python_callable=gradio_interface,\n",
    "        op_kwargs={\"ds\": \"{{ ds }}\"},\n",
    "    )\n",
    "\n",
    "    # Define task dependencies\n",
    "    (\n",
    "        dummy_task\n",
    "        >> create_folders_task\n",
    "        >> task_download_dataset_1\n",
    "        >> split_data_task\n",
    "        >> preprocess_and_train_task\n",
    "        >> gradio_interface_task\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dockerfile\n",
    "\n",
    "```dockerfile\n",
    "# Utiliza una imagen base con Python instalado\n",
    "FROM python:3.10-slim\n",
    "\n",
    "# Establece el directorio de trabajo en el contenedor\n",
    "WORKDIR /root/airflow\n",
    "\n",
    "# Establece la variable de entorno AIRFLOW_HOME\n",
    "ENV AIRFLOW_HOME=/root/airflow\n",
    "\n",
    "# Instalar dependencias del sistema\n",
    "RUN apt-get update && apt-get install -y \\\n",
    "    curl \\\n",
    "    && apt-get clean \\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# Actualizar pip\n",
    "RUN python -m pip install --upgrade pip\n",
    "\n",
    "# Instala Apache Airflow y otras librer铆as\n",
    "COPY requirements.txt .\n",
    "RUN pip install -r requirements.txt\n",
    "\n",
    "# Inicializa la base de datos de Airflow\n",
    "RUN airflow db migrate\n",
    "\n",
    "\n",
    "# Crea el usuario admin de Airflow\n",
    "RUN airflow users create --role Admin --username admin --email admin \\\n",
    " --firstname admin --lastname admin --password admin\n",
    "\n",
    "# Copia las carpetas necesarias al contenedor\n",
    "COPY ./dags $AIRFLOW_HOME/dags\n",
    "COPY ./logs $AIRFLOW_HOME/logs\n",
    "COPY ./plugins $AIRFLOW_HOME/plugins\n",
    "\n",
    "# Expone el puerto 8080 para el servidor web de Airflow\n",
    "EXPOSE 8080 7860\n",
    "\n",
    "# Comando para iniciar el servidor web y el scheduler\n",
    "CMD [\"sh\", \"-c\", \"airflow webserver -p 8080 & airflow scheduler\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### requirements.txt\n",
    "```\n",
    "apache-airflow==2.11.0\n",
    "apache-airflow-providers-fab\n",
    "scikit-learn\n",
    "datetime\n",
    "pandas\n",
    "numpy\n",
    "joblib\n",
    "gradio\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KqBlHcBQpXJb"
   },
   "source": [
    "# **2. Paralelizando el Pipeline** (30 puntos)\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://i.gifer.com/8LNL.gif\" width=\"400\">\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SoQaVOeiqO_R"
   },
   "source": [
    "Al ver los resultados obtenidos, Vale queda muy contenta con el clasificador. Sin embargo, le aparecen algunas dudas respecto al funcionamiento del pipeline. Primero le comenta que es posible que en un futuro tenga nuevos datos que podr铆an ser 煤tiles para realizar nuevos entrenamientos, por lo que ser铆a ideal si este pipeline se fuera ejecutando de forma peri贸dica y **NO** de forma manual. Adem谩s, Vale le menciona que le gustar铆a explorar el desempe帽o de otros modelos adem谩s de `Random Forest`, de forma que el pipeline seleccione de forma autom谩tica el modelo con mejor desempe帽o para luego hacer la predicci贸n de Vale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9mGPMg0ur-wR"
   },
   "source": [
    "## **2.1 Preparando un Nuevo Pipeline** (15 puntos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fpU81VCRr-Hr"
   },
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"https://media.tenor.com/gnA7-5TewXMAAAAM/elle-woods.gif\" width=\"400\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7KcXuS6bsZAw"
   },
   "source": [
    "De acuerdo a lo que le coment贸 Vale, usted decide crear un nuevo script con las funciones que utilizar谩 su pipeline. Por ende, dentro de la carpeta `dags`, usted crear谩 el archivo `hiring_dynamic_functions.py` el cual debe contener:\n",
    "\n",
    "1. (2 puntos) Una funci贸n llamada `create_folders()` que cree una carpeta, la cual utilice la fecha de ejecuci贸n como nombre. Adicionalmente, dentro de esta carpeta debe crear las siguientes subcarpetas:\n",
    "  - raw\n",
    "  - preprocessed\n",
    "  - splits\n",
    "  - models\n",
    "2. (2 puntos) Una funci贸n llamada `load_ands_merge()` que lea desde la carpeta `raw` los archivos `data_1.csv`y `data_2.csv` en caso de estar disponible. Luego concatene estos y genere un nuevo archivo resultante, guard谩ndolo en la carpeta `preprocessed`.\n",
    "\n",
    "3. (2 puntos) Una funci贸n llamada `split_data()` que lea la data guardada en la carpeta `preprocessed` y realice un hold out sobre esta data. Esta funci贸n debe crear un conjunto de entrenamiento y uno de prueba. Mantenga una semilla y 20% para el conjunto de prueba. Guarde los conjuntos resultantes en la carpeta `splits`.\n",
    "\n",
    "4. (6 puntos) Una funci贸n llamada `train_model()` que reciba un modelo de clasificaci贸n.\n",
    "    - La funci贸n debe comenzar leyendo el conjunto de entrenamiento desde la carpeta `spits`.\n",
    "    - Esta debe crear y aplicar un `Pipeline` con una etapa de preprocesamiento. Utilice `ColumnTransformers` para aplicar las transformaciones que estime convenientes.\n",
    "    - A帽ada una etapa de entrenamiento utilizando un modelo que ingrese a la funci贸n.\n",
    "  \n",
    "  Esta funci贸n **debe crear un archivo joblib con el pipeline entrenado**. Guarde el modelo con un nombre que le permita una facil identificaci贸n dentro de la carpeta `models`.\n",
    "\n",
    "5. (3 puntos) Una funci贸n llamada `evaluate_models()` que reciba sus modelos entrenados desde la carpeta `models`, eval煤e su desempe帽o mediante `accuracy` en el conjunto de prueba y seleccione el mejor modelo obtenido. Luego guarde el mejor modelo como archivo `.joblib`. Su funci贸n debe imprimir el nombre del modelo seleccionado y el accuracy obtenido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KnX61hxjW9rI"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import gradio as gr\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from airflow.models import Variable\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "\n",
    "\n",
    "def create_folders(ds, **kwargs):\n",
    "    base_path = f\"/root/airflow/{ds}\"\n",
    "    os.makedirs(f\"{base_path}/raw\", exist_ok=True)\n",
    "    os.makedirs(f\"{base_path}/preprocessed\", exist_ok=True)\n",
    "    os.makedirs(f\"{base_path}/splits\", exist_ok=True)\n",
    "    os.makedirs(f\"{base_path}/models\", exist_ok=True)\n",
    "\n",
    "\n",
    "def load_ands_merge(ds, **kwargs):\n",
    "    base_path = f\"/root/airflow/{ds}\"\n",
    "    # Leer archivo data_1.csv desde la carpeta raw\n",
    "    df1 = pd.read_csv(f\"{base_path}/raw/data_1.csv\")\n",
    "    # Leer archivo data_2.csv desde la carpeta raw si es que esta disponible\n",
    "    df2 = (\n",
    "        pd.read_csv(f\"{base_path}/raw/data_2.csv\")\n",
    "        if os.path.exists(f\"{base_path}/raw/data_2.csv\")\n",
    "        else pd.DataFrame()\n",
    "    )\n",
    "    # concatenar los dataframes\n",
    "    df = pd.concat([df1, df2], ignore_index=True)\n",
    "    # Guardar el dataframe combinado\n",
    "    df.to_csv(f\"{base_path}/preprocessed/combined_data.csv\", index=False)\n",
    "\n",
    "\n",
    "def split_data(ds, **kwargs):\n",
    "    base_path = f\"/root/airflow/{ds}\"\n",
    "    df = pd.read_csv(f\"{base_path}/preprocessed/combined_data.csv\")\n",
    "    train_df, test_df = train_test_split(\n",
    "        df, test_size=0.2, stratify=df[\"HiringDecision\"], random_state=1892\n",
    "    )\n",
    "    # Guardar los conjuntos divididos en la carpeta splits\n",
    "    train_df.to_csv(f\"{base_path}/splits/train.csv\", index=False)\n",
    "    test_df.to_csv(f\"{base_path}/splits/test.csv\", index=False)\n",
    "\n",
    "\n",
    "def train_model(ds, model, **kwargs):\n",
    "    base_path = f\"/root/airflow/{ds}\"\n",
    "    # Leer los conjuntos de entrenamiento y prueba desde la carpeta splits\n",
    "    train_df = pd.read_csv(f\"{base_path}/splits/train.csv\")\n",
    "    test_df = pd.read_csv(f\"{base_path}/splits/test.csv\")\n",
    "\n",
    "    # Separar caracter铆sticas y etiquetas\n",
    "    X_train = train_df.drop(\"HiringDecision\", axis=1)\n",
    "    y_train = train_df[\"HiringDecision\"]\n",
    "    X_test = test_df.drop(\"HiringDecision\", axis=1)\n",
    "    y_test = test_df[\"HiringDecision\"]\n",
    "\n",
    "    # Pipeline de preprocesamiento\n",
    "    numeric_features = [\n",
    "        \"Age\",\n",
    "        \"DistanceFromCompany\",\n",
    "        \"InterviewScore\",\n",
    "        \"SkillScore\",\n",
    "        \"PersonalityScore\",\n",
    "    ]\n",
    "    ordinal_features = [\"EducationLevel\", \"ExperienceYears\"]\n",
    "\n",
    "    categorical_features = [\"Gender\", \"RecruitmentStrategy\", \"PreviousCompanies\"]\n",
    "\n",
    "    # Pipeline de preprocesamiento\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", StandardScaler(), numeric_features),\n",
    "            (\"ord\", OrdinalEncoder(), ordinal_features),\n",
    "            (\n",
    "                \"cat\",\n",
    "                OneHotEncoder(drop=\"first\", sparse_output=False),\n",
    "                categorical_features,\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    # Pipeline completo: preprocesamiento + modelo\n",
    "    pipeline = Pipeline(\n",
    "        [\n",
    "            (\"preprocessor\", preprocessor),\n",
    "            (\"classifier\", model),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Guardar el modelo entrenado\n",
    "    joblib.dump(pipeline, f\"{base_path}/models/hiring_{model.__class__.__name__}.pkl\")\n",
    "\n",
    "\n",
    "def evaluate_models(ds, **kwargs):\n",
    "    base_path = f\"/root/airflow/{ds}\"\n",
    "    test_df = pd.read_csv(f\"{base_path}/splits/test.csv\")\n",
    "    X_test = test_df.drop(\"HiringDecision\", axis=1)\n",
    "    y_test = test_df[\"HiringDecision\"]\n",
    "\n",
    "    best_model = None\n",
    "    best_accuracy = 0.0\n",
    "    best_model_name = \"\"\n",
    "\n",
    "    for model_file in os.listdir(f\"{base_path}/models\"):\n",
    "        if model_file.endswith(\".pkl\"):\n",
    "            model_path = os.path.join(f\"{base_path}/models\", model_file)\n",
    "            model = joblib.load(model_path)\n",
    "            y_pred = model.predict(X_test)\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            print(f\"Model: {model_file}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_model = model\n",
    "                best_model_name = model_file.strip(\".pkl\")\n",
    "\n",
    "    if best_model is not None:\n",
    "        joblib.dump(best_model, f\"{base_path}/models/best_hiring_model.pkl\")\n",
    "        print(f\"Best Model: {best_model_name} with Accuracy: {best_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RUYkXWcZJz3b"
   },
   "source": [
    "## **2.2 Componiendo un nuevo DAG** (15 puntos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ak7uL9YXJ6Xj"
   },
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"https://67.media.tumblr.com/bfa5208006dc3f404ec08e8c3195cf2c/tumblr_obg9tgnLfX1u9e9f2o2_r1_500.gif\" width=\"500\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NbE6mu20LfWd"
   },
   "source": [
    "Con las nuevas funciones, se debe crear el nuevo nuevo DAG. Para ello, cree un nuevo script en la carpeta `dags`, llamandolo `dag_dynamic.py`. Este script debe contener la siguiente estructura:\n",
    "\n",
    "1. (1 punto) Inicialice un DAG con fecha de inicio el 1 de octubre de 2024, el cual se debe ejecutar el d铆a 5 de cada mes a las 15:00 UTC. Utilice un `dag_id` interpretable para identificar f谩cilmente. **Habilite el backfill** para que pueda ejecutar tareas programadas desde fechas pasadas.\n",
    "2. (1 punto) Comience con un marcador de posici贸n que indique el inicio del pipeline.\n",
    "3. (2 puntos) Cree una carpeta correspondiente a la ejecuci贸n del pipeline y cree las subcarpetas `raw`, `preprocessed`, `splits` y `models` mediante la funci贸n `create_folders()`.\n",
    "4. (2 puntos) Implemente un `Branching`que siga la siguiente l贸gica:\n",
    "  - Fechas previas al 1 de noviembre de 2024: Se descarga solo `data_1.csv`\n",
    "  - Desde el 1 de noviembre del 2024: descarga `data_1.csv` y `data_2.csv`.\n",
    "  En el siguiente [enlace](https://gitlab.com/eduardomoyab/laboratorio-13/-/raw/main/files/data_2.csv) puede descargar `data_2.csv`.\n",
    "5. (1 punto) Cree una tarea que concatene los datasets disponibles mediante la funci贸n `load_and_merge()`. Configure un `Trigger` para que la tarea se ejecute si encuentra disponible **como m铆nimo** uno de los archivos.\n",
    "6. (1 punto) Aplique el hold out al dataset mediante la funci贸n `split_data()`, obteniendo un conjunto de entrenamiento y uno de prueba.\n",
    "7. (2 puntos) Realice 3 entrenamientos en paralelo:\n",
    "  - Un modelo Random Forest.\n",
    "  - 2 modelos a elecci贸n.\n",
    "  Aseg煤rese de guardar sus modelos entrenados con nombres distintivos. Utilice su funci贸n `train_model()` para ello.\n",
    "8. (2 puntos) Mediante la funci贸n `evaluate_models()`, eval煤e los modelos entrenados, registrando el accuracy de cada modelo en el set de prueba. Luego debe imprimir el mejor modelo seleccionado y su respectiva m茅trica. Configure un `Trigger` para que la tarea se ejecute solamente si los 3 modelos fueron entrenados y guardados.\n",
    "\n",
    "`Hint:` Recuerde que puede entregar `kwargs` a sus funciones, como por ejemplo la fecha de ejecuci贸n `ds`.\n",
    "\n",
    "Una vez creado el script, vuelva a construir el contenedor en Docker, acceda a la aplicaci贸n web de Airflow, ejecute su pipeline y muestre sus resultados. Adjunte im谩genes que ayuden a mostrar el proceso y sus resultados.\n",
    "\n",
    "Adicionalmente, responda (1 c/u):\n",
    "\n",
    "- 驴Cual es el accuracy de cada modelo en la ejecuci贸n de octubre? 驴Se obtienen los mismos resultados a partir de Noviembre?\n",
    "- Analice como afect贸 el a帽adir datos a sus modelos mediante el desempe帽o del modelo y en costo computacional.\n",
    "- Muestre el esquema de su DAG ejecutado en octubre y en noviembre.\n",
    "\n",
    "\n",
    "`Nota:` Para esta secci贸n no debe implementar la tarea en gradio, solamente se espera determinar el mejor modelo y comparar el desempe帽o obtenido.\n",
    "\n",
    "**IMPORTANTE: Para esta secci贸n, debe adjuntar todos los scripts creados junto a su notebook en la entrega, ya que ser谩n ejecutados para validar el funcionamiento. Para justificar sus respuestas, adicionaslmente puede utilizar im谩genes de apoyo, como screenshots.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cMgK2sKTYJji"
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.operators.empty import EmptyOperator\n",
    "from airflow.operators.python import BranchPythonOperator, PythonOperator\n",
    "from airflow.utils.dates import days_ago\n",
    "from hiring_dynamic_functions import (\n",
    "    create_folders,\n",
    "    evaluate_models,\n",
    "    load_and_merge,\n",
    "    split_data,\n",
    "    train_model,\n",
    ")\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "default_args = {\n",
    "    \"owner\": \"free-riders\",\n",
    "    \"retries\": 1,\n",
    "}\n",
    "\n",
    "with DAG(\n",
    "    dag_id=\"hiring_dynamic\",\n",
    "    default_args=default_args,\n",
    "    description=\"Hiring Decision Workflow with Dynamic Tasks\",\n",
    "    start_date=dt.datetime(2024, 10, 1),\n",
    "    schedule_interval=\"0 15 5 * *\",  # cada dia 5 del mes a las 15:00 UCT\n",
    "    catchup=True,\n",
    ") as dag:\n",
    "\n",
    "    # Task 1 - Inicial dummy task\n",
    "    dummy_task = EmptyOperator(task_id=\"Starting_the_process\", retries=2)\n",
    "\n",
    "    # Task 2 - Crear las carpetas necesarias\n",
    "    create_folders_task = PythonOperator(\n",
    "        task_id=\"create_folders\",\n",
    "        python_callable=create_folders,\n",
    "        op_kwargs={\"ds\": \"{{ ds }}\"},\n",
    "    )\n",
    "\n",
    "    # Branching: Antes del 1-11-2024 descargar solo data_1.csv, despues ambas\n",
    "    branch_task = BranchPythonOperator(\n",
    "        task_id=\"branch_task\",\n",
    "        python_callable=lambda **kwargs: (\n",
    "            \"download_dataset_1\"\n",
    "            if kwargs['execution_date'] < dt.datetime(2024, 11, 1, tzinfo=kwargs['execution_date'].tzinfo)\n",
    "            else [\"download_dataset_1\", \"download_dataset_2\"]\n",
    "        ),\n",
    "        provide_context=True,\n",
    "    )\n",
    "\n",
    "    # Task 3a - Descargar dataset 1\n",
    "    task_download_dataset_1 = BashOperator(\n",
    "        task_id=\"download_dataset_1\",\n",
    "        bash_command=\"curl -o \"\n",
    "        \"/root/airflow/{{ ds }}/raw/data_1.csv \"\n",
    "        \"https://gitlab.com/eduardomoyab/laboratorio-13/-/raw/main/files/data_1.csv\",\n",
    "    )\n",
    "\n",
    "    # Task 3b - Descargar dataset 2\n",
    "    task_download_dataset_2 = BashOperator(\n",
    "        task_id=\"download_dataset_2\",\n",
    "        bash_command=\"curl -o \"\n",
    "        \"/root/airflow/{{ ds }}/raw/data_2.csv \"\n",
    "        \"https://gitlab.com/eduardomoyab/laboratorio-13/-/raw/main/files/data_2.csv\",\n",
    "    )\n",
    "\n",
    "    # Task 4 - Load and merge data con Trigger  para ejecutar si encuentra como minimo un dataset\n",
    "    load_and_merge_task = PythonOperator(\n",
    "        task_id=\"load_and_merge\",\n",
    "        python_callable=load_and_merge,\n",
    "        op_kwargs={\"ds\": \"{{ ds }}\"},\n",
    "        trigger_rule=\"one_success\",\n",
    "    )\n",
    "\n",
    "    # Task 5 - Split data\n",
    "    split_data_task = PythonOperator(\n",
    "        task_id=\"split_data\", python_callable=split_data, op_kwargs={\"ds\": \"{{ ds }}\"}\n",
    "    )\n",
    "\n",
    "    # Task 6 - Train models in parallel\n",
    "    models = [\n",
    "        RandomForestClassifier(random_state=1892, n_estimators=100),\n",
    "        GradientBoostingClassifier(random_state=1892),\n",
    "        LogisticRegression(max_iter=200),\n",
    "    ]\n",
    "    train_model_tasks = []\n",
    "    for i, model in enumerate(models):\n",
    "        task = PythonOperator(\n",
    "            task_id=f\"train_model_{i+1}\",\n",
    "            python_callable=train_model,\n",
    "            op_kwargs={\"ds\": \"{{ ds }}\", \"model\": model},\n",
    "        )\n",
    "        train_model_tasks.append(task)\n",
    "\n",
    "    # Task 7 - Evaluate models\n",
    "    evaluate_models_task = PythonOperator(\n",
    "        task_id=\"evaluate_models\",\n",
    "        python_callable=evaluate_models,\n",
    "        op_kwargs={\"ds\": \"{{ ds }}\"},\n",
    "        trigger_rule=\"all_success\",\n",
    "    )\n",
    "\n",
    "    # Setting up dependencies\n",
    "    (\n",
    "        dummy_task\n",
    "        >> create_folders_task\n",
    "        >> branch_task\n",
    "        >> [task_download_dataset_1, task_download_dataset_2]\n",
    "        >> load_and_merge_task\n",
    "        >> split_data_task\n",
    "        >> train_model_tasks\n",
    "        >> evaluate_models_task\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados de Ejecuci贸n\n",
    "\n",
    "#### 1. 驴Cual es el accuracy de cada modelo en la ejecuci贸n de octubre? 驴Se obtienen los mismos resultados a partir de Noviembre?\n",
    "\n",
    "**Ejecuci贸n de Octubre (2024-10-05 - solo data_1.csv):**\n",
    "\n",
    "![Accuracy Octubre](imagenes/accuracy_october.png)\n",
    "\n",
    "Los resultados muestran que en la ejecuci贸n de octubre, donde el pipeline solo utiliz贸 el archivo data_1.csv, el modelo RandomForestClassifier obtuvo un accuracy de 0.8867, mientras que GradientBoostingClassifier alcanz贸 0.8933, convirti茅ndose en el mejor modelo de esta ejecuci贸n. Por su parte, LogisticRegression present贸 el desempe帽o m谩s bajo con 0.8600.\n",
    "\n",
    "**Ejecuci贸n de Noviembre (2024-11-05 - data_1.csv + data_2.csv):**\n",
    "\n",
    "![Accuracy Noviembre](imagenes/accuracy_november.png)\n",
    "\n",
    "En contraste, la ejecuci贸n de noviembre, que incorpor贸 tanto data_1.csv como data_2.csv, mostr贸 resultados notablemente diferentes. RandomForestClassifier alcanz贸 0.9300, GradientBoostingClassifier obtuvo 0.9233, y LogisticRegression mejor贸 levemente a 0.8667. Es importante destacar que el mejor modelo cambi贸 de GradientBoosting en octubre a RandomForest en noviembre.\n",
    "\n",
    "**An谩lisis:**\n",
    "\n",
    "Los resultados claramente demuestran que no se obtienen los mismos valores entre ambas ejecuciones. La incorporaci贸n de datos adicionales gener贸 una mejora generalizada en todos los modelos, siendo RandomForest el que experiment贸 el incremento m谩s significativo con 4.33 puntos porcentuales. Este comportamiento sugiere que RandomForest posee una mayor capacidad para aprovechar conjuntos de datos m谩s extensos, permiti茅ndole capturar patrones m谩s complejos y diversos. GradientBoosting tambi茅n mostr贸 una mejora considerable de 3 puntos porcentuales, mientras que LogisticRegression present贸 el menor incremento con apenas 0.67 puntos, lo que podr铆a indicar que los modelos lineales tienen limitaciones para beneficiarse de datos adicionales cuando existen relaciones no lineales en los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Analice como afect贸 el a帽adir datos a sus modelos mediante el desempe帽o del modelo y en costo computacional\n",
    "\n",
    "**Desempe帽o del Modelo:**\n",
    "\n",
    "La incorporaci贸n de datos adicionales tuvo un impacto significativo en el desempe帽o de los modelos evaluados. Al comparar los resultados de octubre y noviembre, se observa que todos los clasificadores experimentaron mejoras en sus m茅tricas de accuracy, aunque en magnitudes diferentes. RandomForest demostr贸 ser el modelo que m谩s se benefici贸 de la ampliaci贸n del conjunto de datos, pasando de 0.8867 a 0.9300, lo que representa un incremento de 4.33 puntos porcentuales. Esta mejora no solo fue la m谩s pronunciada, sino que tambi茅n permiti贸 que RandomForest superara a GradientBoosting como el mejor modelo del pipeline. Por su parte, GradientBoosting tambi茅n mostr贸 una mejora sustancial de 3 puntos porcentuales, mientras que LogisticRegression present贸 el menor incremento con apenas 0.67 puntos.\n",
    "\n",
    "Este comportamiento diferenciado entre los modelos puede atribuirse a sus caracter铆sticas intr铆nsecas. Los modelos basados en ensambles de 谩rboles, como RandomForest y GradientBoosting, poseen una mayor capacidad para capturar relaciones complejas y no lineales en los datos, lo que les permite aprovechar mejor conjuntos de datos m谩s extensos. En contraste, LogisticRegression, al ser un modelo lineal, muestra limitaciones inherentes para beneficiarse de datos adicionales cuando los patrones subyacentes son predominantemente no lineales. El incremento de datos permiti贸 una mejor generalizaci贸n de los modelos, alcanzando el mejor resultado un accuracy de 93%, lo que representa una mejora notable respecto al 89% obtenido con el conjunto de datos m谩s reducido.\n",
    "\n",
    "**Costo Computacional:**\n",
    "\n",
    "El an谩lisis del costo computacional revela informaci贸n interesante sobre la escalabilidad del pipeline implementado. La ejecuci贸n completa del DAG en octubre tom贸 4 minutos y 15 segundos, mientras que en noviembre el tiempo se extendi贸 a 5 minutos y 18 segundos. Este incremento incluye no solo el tiempo adicional de entrenamiento de los modelos con m谩s datos, sino tambi茅n las operaciones de descarga del segundo archivo, el merge de ambos datasets, y el preprocesamiento de un volumen mayor de informaci贸n.\n",
    "\n",
    "Lo destacable de estos resultados es que, a pesar de aproximadamente duplicar el tama帽o del dataset de entrenamiento, el tiempo de ejecuci贸n solo aument贸 en aproximadamente un cuarto. Esto sugiere que el pipeline escala de manera eficiente y que la arquitectura implementada maneja adecuadamente incrementos en el volumen de datos.\n",
    "\n",
    "**Trade-off y Conclusi贸n:**\n",
    "\n",
    "Al evaluar la relaci贸n entre costo y beneficio, queda claro que la incorporaci贸n de datos adicionales representa una decisi贸n altamente favorable desde el punto de vista del rendimiento del pipeline. Por un lado, se obtuvo una ganancia en accuracy para el mejor modelo, elevando la precisi贸n de 88.67% a 93%. Por otro lado, el costo asociado fue 煤nicamente un minuto adicional de procesamiento, lo que en t茅rminos pr谩cticos resulta despreciable para la mayor铆a de aplicaciones productivas. La relaci贸n costo-beneficio es particularmente atractiva: por cada minuto extra de ejecuci贸n, se obtiene una mejora significativa en la calidad de las predicciones. En un entorno productivo donde la precisi贸n del modelo puede traducirse directamente en mejor toma de decisiones para Vale, este incremento marginal en tiempo de procesamiento es completamente justificable y representa una inversi贸n eficiente de recursos computacionales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Muestre el esquema de su DAG ejecutado en octubre y en noviembre\n",
    "\n",
    "**Esquema del DAG - Ejecuci贸n de Octubre:**\n",
    "\n",
    "![DAG Graph Octubre](imagenes/graph_october.png)\n",
    "\n",
    "En la ejecuci贸n de octubre, el branching implementado en el DAG eval煤a la fecha de ejecuci贸n y determina que, al ser anterior al 1 de noviembre de 2024, solo debe descargar el archivo data_1.csv. Esto se refleja en el grafo donde 煤nicamente la tarea `download_dataset_1` se ejecuta, mientras que `download_dataset_2` permanece en estado omitido (skipped). El flujo contin煤a normalmente con las tareas de merge, split, entrenamiento paralelo de los tres modelos y evaluaci贸n final.\n",
    "\n",
    "**Esquema del DAG - Ejecuci贸n de Noviembre:**\n",
    "\n",
    "![DAG Graph Noviembre](imagenes/graph_november.png)\n",
    "\n",
    "Por el contrario, en la ejecuci贸n de noviembre, el mecanismo de branching detecta que la fecha de ejecuci贸n es posterior al 1 de noviembre de 2024, por lo que decide ejecutar ambas tareas de descarga: `download_dataset_1` y `download_dataset_2`. Esta diferencia es crucial y se aprecia claramente en el grafo, donde ambas tareas aparecen en estado de ejecuci贸n exitosa. Posteriormente, la tarea de merge combina ambos archivos antes de proceder con el resto del pipeline.\n",
    "\n",
    "**An谩lisis:**\n",
    "\n",
    "La diferencia fundamental entre ambas ejecuciones reside en el comportamiento del operador `BranchPythonOperator` implementado en la tarea `branch_task`. Este operador eval煤a din谩micamente la fecha de ejecuci贸n mediante `kwargs['execution_date']` y compara esta fecha con el umbral establecido del 1 de noviembre de 2024. Para fechas anteriores, retorna 煤nicamente el identificador de la tarea `download_dataset_1`, mientras que para fechas posteriores retorna una lista conteniendo ambos identificadores de descarga. Este mecanismo permite que el pipeline se adapte autom谩ticamente al contexto temporal, descargando solo los datos disponibles en cada momento sin requerir intervenci贸n manual.\n",
    "\n",
    "Es importante destacar que, independientemente de la rama tomada en el branching, el resto del pipeline permanece estructuralmente id茅ntico. La tarea `load_and_merge` est谩 configurada con `trigger_rule=\"one_success\"`, lo que garantiza su ejecuci贸n siempre que al menos una de las tareas de descarga se complete exitosamente. Esto proporciona robustez al pipeline, asegurando que pueda continuar operando incluso si solo uno de los archivos est谩 disponible. Posteriormente, las tareas de split, entrenamiento paralelo y evaluaci贸n se ejecutan de manera id茅ntica en ambos escenarios, procesando el dataset resultante independientemente de si proviene de uno o dos archivos fuente. Esta arquitectura flexible y resiliente demuestra las capacidades de Airflow para implementar l贸gica condicional compleja manteniendo la claridad y mantenibilidad del c贸digo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wrmM65RIRrgm"
   },
   "source": [
    "# Conclusi贸n\n",
    "\n",
    "xito!\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://miro.medium.com/v2/resize:fit:1000/1*PX8WVijZapo7EDrvGv9Inw.gif\" width=\"500\">\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "mds7202-free-riders",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
