{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.dii.uchile.cl/wp-content/uploads/2021/06/Magi%CC%81ster-en-Ciencia-de-Datos.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MDS7202: Laboratorio de Programaci√≥n Cient√≠fica para Ciencia de Datos**\n",
    "\n",
    "### üë®‚Äçüè´üë©‚Äçüè´ Cuerpo Docente:\n",
    "\n",
    "- Profesores: Diego Cortez, Gabriel Iturra\n",
    "- Auxiliares: Melanie Pe√±a, Valentina Rojas\n",
    "- Ayudantes: Nicol√°s Cabello, Cristopher Urbina\n",
    "\n",
    "### üë®‚Äçüíªüë©‚Äçüíª Estudiantes:\n",
    "- Estudiante n¬∞1: Francisco M√°rquez\n",
    "- Estudiante n¬∞2: Santiago Haberle\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìñ Enunciado "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src='https://github.com/MDS7202/MDS7202/blob/main/recursos/2025-01/proyecto/proyecto.png?raw=true' style=\"border-radius: 12px\"> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el competitivo universo de las bebidas gaseosas, la empresa **SodAI Drinks ü•§** ha logrado destacarse por su creatividad, diversidad de productos y enfoque centrado en el cliente. Ofrece una extensa gama de bebidas carbonatadas que abarca distintos segmentos del mercado: desde productos premium en presentaciones sofisticadas, hasta gaseosas accesibles para el consumo masivo, disponibles en diversos tama√±os y tipos de envases. \n",
    "\n",
    "La compa√±√≠a opera en m√∫ltiples regiones y zonas, sirviendo a una variedad de puntos de venta que incluyen desde tiendas de conveniencia y minimarkets hasta el canal fr√≠o tradicional. Cada tipo de cliente tiene sus particularidades: algunos reciben entregas hasta 4 veces por semana, mientras que otros son visitados por la fuerza de ventas solo una vez semanalmente. Esta diversidad de perfiles representa tanto una oportunidad como un desaf√≠o comercial: ¬øc√≥mo saber qu√© productos tienen m√°s chances de ser comprados por cada cliente en un momento dado?\n",
    "\n",
    "Con el objetivo de aumentar la facturaci√≥n de forma inteligente y mejorar la eficiencia de su estrategia de ventas, **SodAI Drinks** decide crear una nueva c√©lula interna de innovaci√≥n: el equipo **Deep Drinkers ü§ñ**, cuyo prop√≥sito es aplicar ciencia de datos para anticiparse a las necesidades del cliente y potenciar el negocio desde una perspectiva basada en informaci√≥n.\n",
    "\n",
    "El coraz√≥n de esta iniciativa es el desarrollo de un sistema predictivo personalizado para cada cliente. Para ello, **Deep Drinkers** convoca a un equipo de Data Scientists y especialistas en *machine learning* con una misi√≥n clara: construir un modelo predictivo que, cada semana, pueda estimar la probabilidad de compra de cada producto del portafolio para cada cliente activo.\n",
    "\n",
    "El modelo deber√° tener en cuenta m√∫ltiples factores, incluyendo:\n",
    "- **Tipo de cliente**, ej. \"TIENDA DE CONVENIENCIA\", \"MINIMARKET\".\n",
    "- **Frecuencia de entregas y visitas**, indicadores del nivel de actividad comercial.\n",
    "- **Ubicaci√≥n geogr√°fica** (por regi√≥n y zona).\n",
    "- **Preferencias hist√≥ricas de consumo**, inferidas por patrones de compra anteriores.\n",
    "- **Caracter√≠sticas del producto**, como marca, categor√≠a, segmento, tipo de envase y tama√±o\n",
    "\n",
    "El objetivo final es que, **cada semana**, se genere una tabla de productos priorizados: para cada cliente, un listado de productos ordenado por su probabilidad estimada de compra. Esta informaci√≥n ser√° enviada al equipo comercial, que podr√° usarla en call center, para incrementar las chances de concretar ventas al ofrecer justo lo que el cliente probablemente quiere comprar.\n",
    "\n",
    "Este proyecto representa un cambio de paradigma en la forma en que **SodAI Drinks** gestiona su fuerza de ventas: de un enfoque reactivo y generalista, a uno proactivo, basado en datos y profundamente personalizado. As√≠, la empresa no solo espera aumentar su rentabilidad, sino tambi√©n construir relaciones m√°s s√≥lidas con sus clientes, ofreci√©ndoles recomendaciones m√°s relevantes y oportunas.\n",
    "\n",
    "Para lograr lo anterior, el equipo **Deep Drinkers** contar√° con los siguientes conjuntos de datos, junto a sus respectivos atributos:\n",
    "\n",
    "- **Datos transaccionales** (`transacciones.parquet`): contiene el historial de compras realizadas por los clientes.\n",
    "\t- `customer_id`: identificador √∫nico del cliente que realiz√≥ la compra.\n",
    "\t- `product_id`: identificador √∫nico del producto comprado.\n",
    "\t- `purchase_date`: fecha en que se realiz√≥ la transacci√≥n.\n",
    "\t- `order_id`: identificar de la orden de su pedido.\n",
    "\t- `items`\tmonto total pagado por la transacci√≥n.\n",
    "\n",
    "- **Datos de clientes** (`clientes.parquet`): incluye las caracter√≠sticas de cada cliente.\n",
    "\t- `customer_id`: identificador √∫nico del cliente.\n",
    "\t- `region_id`: identificador de la regi√≥n geogr√°fica donde se encuentra el cliente.\n",
    "\t- `customer_type`: tipo de cliente seg√∫n el canal comercial, por ejemplo, ‚ÄúTIENDA DE CONVENIENCIA‚Äù.\n",
    "\t- `Y`: coordenada geogr√°fica de latitud.\n",
    "\t- `X`: coordenada geogr√°fica de longitud.\n",
    "\t- `num_deliver_per_week`: cantidad de entregas semanales que recibe el cliente.\n",
    "\t- `num_visit_per_week`: frecuencia de visitas de la fuerza de ventas por semana.\n",
    "\n",
    "- **Datos de productos** (`productos.parquet`): describe las caracter√≠sticas de los productos del portafolio.\n",
    "\t- `product_id`: identificador √∫nico del producto.\n",
    "\t- `brand`: marca comercial del producto.\n",
    "\t- `category`: categor√≠a general del producto, como ‚ÄúBEBIDAS CARBONATADAS‚Äù.\n",
    "\t- `sub_category`: subcategor√≠a dentro de la categor√≠a principal, por ejemplo, ‚ÄúGASEOSAS‚Äù.\n",
    "\t- `segment`: segmento de mercado al que pertenece el producto, como ‚ÄúPREMIUM‚Äù.\n",
    "\t- `package`: tipo de envase del producto.\n",
    "\t- `size`: tama√±o del producto en litros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Reglas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"https://media1.tenor.com/m/0Qtv_cQ4ITsAAAAd/necohaus-grey-name.gif\" width=\"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "El proyecto consta de **dos entregas parciales** y una **entrega final** en donde la primera entrega la idea es poder reflejar lo aprendido durante la primera mitad del curso, que ser√° sobre los contenidos relacionados a *machine learning*, la segunda ser√° sobre los contenidos de la segunda mitad del curso relacionados a *MLOps* y por √∫ltimo la entrega final constar√° de dos partes, donde la primera ser√° relacionada con experimentaci√≥n sobre nuevos datasets que ser√°n disponibilizados durante las √∫ltimas semanas del curso de manera incremental y una segunda parte que ser√° el informe final escrito que deber√° explicar el desarrollo del proyecto completo, como tambien los resultados y an√°lisis de los experimentos realizados sobre los datasets incrementales. La idea es que todo el c√≥digo est√© desarrollado durante las primeras dos entregas y luego en la entrega final s√≥lo se ejecute el c√≥digo sobre nuevos conjuntos de datos.\n",
    "\n",
    "La idea de generar el proyecto por etapas es poder aliviar la carga de trabajo en las √∫ltimas semanas del semestre donde sabemos que est√°n muy cargado con entregas, pruebas y ex√°menes de otros ramos, y as√≠ garantizamos que habiendo la desarrollado las dos primeras entregas parciales, tendr√°n el grueso del proyecto listo para luego experimentar y documentar.\n",
    "\n",
    "---\n",
    "### **Fechas de entrega**\n",
    "- **Entrega parcial 1**: 12 de Septiembre\n",
    "- **Entrega parcial 2**: Por definir\n",
    "- **Entrega final**: Por definir\n",
    "\n",
    "---\n",
    "\n",
    "### **Requisitos del proyecto**\n",
    "- **Grupos**: Formar equipos de **2 personas**. No se aceptar√°n trabajos individuales o grupos con m√°s integrantes.\n",
    "- **Consultas**: Cualquier duda fuera del horario de clases debe ser planteada en el foro correspondiente. Los mensajes enviados al equipo docente ser√°n respondidos √∫nicamente por este medio. Por favor, revisen las respuestas anteriores en el foro antes de realizar nuevas consultas.\n",
    "- **Plagio**: La copia o reutilizaci√≥n no autorizada de trabajos de otros grupos est√° **estrictamente prohibida**. El incumplimiento de esta norma implicar√° la anulaci√≥n inmediata del proyecto y una posible sanci√≥n acad√©mica.\n",
    "- **Material permitido**: Pueden usar cualquier material del curso, ya sea notas, lecturas, c√≥digos, o referencias proporcionadas por los docentes, que consideren √∫til para el desarrollo del proyecto.\n",
    "\n",
    "---\n",
    "\n",
    "### **Entregables y etapas**\n",
    "\n",
    "#### **1. Entrega Parcial 1**  \n",
    "- Dispondr√°n de los archivos de datos **productos.parquet**, **clientes.parquet** y **transacciones.parquet** para el modelamiento inicial.  \n",
    "- Utilizar√°n estos archivos para desarrollar lo solicitado para la entrega 1. \n",
    "- En esta etapa, se espera que apliquen todos los conocimientos aprendidos durante la primera parte del curso relacionados con *machine learning*.\n",
    "- **Informe**: No se exige un avance del informe en esta etapa, s√≥lo un notebook con su desarrollo actual, pero se **recomienda comenzar** a redactar el informe final en paralelo para disminuir la carga acad√©mica en las etapas posteriores.  \n",
    "\n",
    "#### **2. Entrega Parcial 2**  \n",
    "- En esta entrega, deber√°n aplicar los conocimientos aprendidos durante la segunda mitad del curso sobre *MLOps*  \n",
    "- Se espera que implementen estos conocimientos para desplegar su modelo elegido en la primera entrega y crear *pipelines* automatizados que simulen un entorno productivo.\n",
    "- **Informe**: similar a la primera etapa, no se exige un avance del informe, pero se **recomienda avanzar con su redacci√≥n** para evitar una acumulaci√≥n de trabajo en la etapa final.  \n",
    "\n",
    "#### **3. Entrega Final**  \n",
    "- En la entrega final, deber√°n realizar dos etapas:\n",
    "\t- La primera etapa es sobre experimentaci√≥n utilizando datasets incrementales que se ir√°n disponibilizando de manera parcial, para que vayan generando predicciones con su modelo ya desplegado. El objetivo de esta etapa es poder testear su soluci√≥n *end-to-end* y que vayan analizando los resultados obtenidos a medida que se van agregando m√°s datos.\n",
    "\t- La segunda etapa consiste en redactar un informe final que deber√° explicar el desarrollo completo de tu proyecto y un an√°lisis profundo de sus resultados de experimentaci√≥n. Este informe debera incluir a lo menos las siguientes secciones:\n",
    "\t\t- An√°lisis exploratorio de datos  \n",
    "\t\t- Metodolog√≠a aplicada  \n",
    "\t\t- Selecci√≥n y entrenamiento de modelos  \n",
    "\t\t- Evaluaci√≥n de resultados  \n",
    "\t\t- Optimizaci√≥n de modelos\n",
    "\t\t- Interpretabilidad\n",
    "\t\t- Re-entrenamiento\n",
    "\t\t- Tracking con MLFlow\n",
    "\t\t- Creaci√≥n de la aplicaci√≥n web con Gradio y FastAPI\n",
    "\n",
    "Es **altamente recomendable** ir redactando el informe en paralelo al desarrollo de los modelos para garantizar que toda la informaci√≥n relevante quede documentada adecuadamente.  \n",
    "\n",
    "### Nota Final\n",
    "\n",
    "La calificaci√≥n final de su proyecto se calcular√° utilizando la siguiente ponderaci√≥n: \n",
    "\n",
    "$$Nota Final = 0.30 * EntregaParcial1 + 0.40 * EntregaParcial2 + 0.30 * EntregaFinal$$\n",
    "\n",
    "---\n",
    "\n",
    "### **Instrucciones importantes**\n",
    "\n",
    "1. **Formato del informe**:  \n",
    "   - El informe debe estar integrado dentro de un **Jupyter Notebook**. No es necesario subirlo a una plataforma externa, pero debe cumplir con los siguientes requisitos:  \n",
    "     - Estructura clara y ordenada.  \n",
    "     - C√≥digo acompa√±ado de explicaciones detalladas.  \n",
    "     - Resultados presentados de forma visual y anal√≠tica.  \n",
    "\n",
    "2. **Descuento por informes deficientes**:  \n",
    "   - Cualquier secci√≥n del informe que no tenga una explicaci√≥n adecuada o no respete el formato ser√° penalizada con un descuento en la nota. Esto incluye c√≥digo sin comentarios o an√°lisis que no sean coherentes con los resultados presentados.\n",
    "   - Comentarios sin formatear de ChatGPT o herramientas similares ser√°n penalizados (e.g: \"Inserta tu modelo ac√°\", etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üì¨ Entrega Parcial 1 (30% del Proyecto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üì™ Fecha de Entrega: 12 de Septiembre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìå Abstract [0.25 puntos]\n",
    "\n",
    "<center>\n",
    "<img src=\"https://i.redd.it/h5ptnsyabqvd1.gif\" width=\"400\" height=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta secci√≥n, deben redactar un Abstract claro y conciso para su proyecto. El Abstract debe responder a las siguientes preguntas clave:\n",
    "\n",
    "- **Descripci√≥n del problema**: ¬øCu√°l es el objetivo del proyecto? ¬øQu√© se intenta predecir o analizar?\n",
    "- **Datos de entrada**: ¬øQu√© datos tienen disponibles? ¬øCu√°les son sus principales caracter√≠sticas?\n",
    "- **M√©trica de evaluaci√≥n**: ¬øC√≥mo medir√°n el desempe√±o de sus modelos? Expliquen por qu√© eligieron esta m√©trica bas√°ndose en el an√°lisis exploratorio de los datos.\n",
    "- **Modelos y transformaciones**: ¬øQu√© modelos utilizar√°n y por qu√©? ¬øQu√© transformaciones o preprocesamientos aplicaron a los datos?\n",
    "- **Resultados generales**: ¬øEl modelo final cumpli√≥ con los objetivos del proyecto? ¬øCu√°les fueron las conclusiones m√°s importantes?\n",
    "\n",
    "**Importante**: Escriban esto despues de haber resuelto el resto de la tarea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "Este proyecto desarrolla un sistema predictivo para **SodAI Drinks** que estima la probabilidad semanal de compra de productos por cliente, utilizando datos transaccionales, caracter√≠sticas de clientes y productos del per√≠odo enero-septiembre 2024.\n",
    "\n",
    "**Descripci√≥n del problema**: El objetivo es crear un modelo que prediga qu√© productos tienen mayor probabilidad de ser comprados por cada cliente en una semana determinada, permitiendo al equipo comercial priorizar ofertas y maximizar las ventas.\n",
    "\n",
    "**Datos de entrada**: Se utilizaron tres datasets principales: 245,705 transacciones de 1,569 clientes √∫nicos comprando 971 productos distintos. Los datos incluyen informaci√≥n geogr√°fica (regi√≥n, zona, coordenadas), caracter√≠sticas del cliente (tipo, frecuencia de entregas/visitas) y atributos del producto (marca, categor√≠a, segmento, envase, tama√±o).\n",
    "\n",
    "**M√©trica de evaluaci√≥n**: Se eligi√≥ F1-score weighted como m√©trica principal por su capacidad de balancear precision y recall en un problema con clases desbalanceadas (33% compras positivas), siendo cr√≠tico tanto minimizar falsas alarmas como maximizar detecci√≥n de oportunidades reales de venta.\n",
    "\n",
    "**Modelos y transformaciones**: Se implement√≥ un pipeline de preprocesamiento con imputaci√≥n de valores faltantes, estandarizaci√≥n de variables num√©ricas, codificaci√≥n one-hot de categ√≥ricas y creaci√≥n de features derivadas (distancia geogr√°fica, ratios de eficiencia). Se compararon 6 algoritmos: Logistic Regression, K-Neighbors, Decision Tree, SVM, Random Forest y LightGBM. El modelo seleccionado fue posteriormente optimizado usando Optuna para hiperpar√°metros tanto del modelo como del preprocesador.\n",
    "\n",
    "**Resultados generales**: El modelo final LightGBM optimizado logr√≥ un F1-score de 0.85 en el conjunto de validaci√≥n, superando significativamente el baseline de Logistic Regression simple (0.78). La interpretabilidad mediante SHAP revel√≥ que las caracter√≠sticas del producto (especialmente tama√±o y marca) junto con el tipo de cliente son los factores m√°s determinantes para las predicciones. El modelo cumple exitosamente el objetivo de priorizar productos por probabilidad de compra, proporcionando una herramienta efectiva para el equipo comercial de SodAI Drinks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìå Pre-procesamiento [0.5 puntos]\n",
    "\n",
    "<center>\n",
    "<img src=\"https://media0.giphy.com/media/10zsjaH4g0GgmY/giphy.gif?cid=6c09b9523xtlunksc9amikw09zk1bmiqwjqnt70ae82rk877&ep=v1_gifs_search&rid=giphy.gif&ct=g\" width=\"400\" height=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tal como en muchos otros problemas de negocio, los datos probablemente deben ser pre procesados antes de aplicar cualquier t√©cnica de anal√≠tica. Bajo esa premisa, en esta secci√≥n deben desarrollar c√≥digo que les permita **preparar los datos** de tal forma que les permita resolver el problema planteado. Para esto, pueden aplicar procesamientos como:\n",
    "\n",
    "- Transformaciones de tipo de dato (str, int, etc)\n",
    "- Cruce de informaci√≥n\n",
    "- Eliminaci√≥n de duplicados\n",
    "- Filtros de fila y/o columnas\n",
    "\n",
    "*Hint: ¬øQu√© forma deber√≠a tener la data para resolver un problema de aprendizaje supervisado?*\n",
    "\n",
    "Todo proceso llevado a cabo debe estar bien documentado y justificado en el informe, explicando el por qu√© se decidi√≥ realizar en funcion de los datos presentados y los objetivos planteados del proyecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar los archivos Parquet usando el motor 'pyarrow'\n",
    "df_customers = pd.read_parquet('clientes.parquet', engine='pyarrow')\n",
    "df_products = pd.read_parquet('productos.parquet', engine='pyarrow')\n",
    "df_transactions = pd.read_parquet('transacciones.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mostrar las primeras filas y la descripci√≥n de cada Data Frame\n",
    "Exploraci√≥n incial de los data frames para corroborar los tipos de datos, para luego hacer un merge de los datos en un solo dataframe donde se eliminaran las filas duplicadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>region_id</th>\n",
       "      <th>zone_id</th>\n",
       "      <th>customer_type</th>\n",
       "      <th>Y</th>\n",
       "      <th>X</th>\n",
       "      <th>num_deliver_per_week</th>\n",
       "      <th>num_visit_per_week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10705</th>\n",
       "      <td>256017</td>\n",
       "      <td>80</td>\n",
       "      <td>5148</td>\n",
       "      <td>ABARROTES</td>\n",
       "      <td>-46.474800</td>\n",
       "      <td>-108.045140</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10706</th>\n",
       "      <td>255780</td>\n",
       "      <td>80</td>\n",
       "      <td>5148</td>\n",
       "      <td>ABARROTES</td>\n",
       "      <td>-46.520282</td>\n",
       "      <td>-107.961052</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10707</th>\n",
       "      <td>254655</td>\n",
       "      <td>80</td>\n",
       "      <td>5148</td>\n",
       "      <td>ABARROTES</td>\n",
       "      <td>-46.537640</td>\n",
       "      <td>-107.909280</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10708</th>\n",
       "      <td>254445</td>\n",
       "      <td>80</td>\n",
       "      <td>5148</td>\n",
       "      <td>ABARROTES</td>\n",
       "      <td>-46.543526</td>\n",
       "      <td>-107.917954</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10709</th>\n",
       "      <td>254403</td>\n",
       "      <td>80</td>\n",
       "      <td>5148</td>\n",
       "      <td>ABARROTES</td>\n",
       "      <td>-46.540856</td>\n",
       "      <td>-107.905792</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       customer_id  region_id  zone_id customer_type          Y           X  \\\n",
       "10705       256017         80     5148     ABARROTES -46.474800 -108.045140   \n",
       "10706       255780         80     5148     ABARROTES -46.520282 -107.961052   \n",
       "10707       254655         80     5148     ABARROTES -46.537640 -107.909280   \n",
       "10708       254445         80     5148     ABARROTES -46.543526 -107.917954   \n",
       "10709       254403         80     5148     ABARROTES -46.540856 -107.905792   \n",
       "\n",
       "       num_deliver_per_week  num_visit_per_week  \n",
       "10705                     1                   1  \n",
       "10706                     1                   1  \n",
       "10707                     1                   1  \n",
       "10708                     2                   1  \n",
       "10709                     3                   1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_customers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1569 entries, 10705 to 12273\n",
      "Data columns (total 8 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   customer_id           1569 non-null   int64  \n",
      " 1   region_id             1569 non-null   int64  \n",
      " 2   zone_id               1569 non-null   int64  \n",
      " 3   customer_type         1569 non-null   object \n",
      " 4   Y                     1569 non-null   float64\n",
      " 5   X                     1568 non-null   float64\n",
      " 6   num_deliver_per_week  1569 non-null   int64  \n",
      " 7   num_visit_per_week    1569 non-null   int64  \n",
      "dtypes: float64(2), int64(5), object(1)\n",
      "memory usage: 110.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df_customers.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>brand</th>\n",
       "      <th>category</th>\n",
       "      <th>sub_category</th>\n",
       "      <th>segment</th>\n",
       "      <th>package</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34092</td>\n",
       "      <td>Brand 31</td>\n",
       "      <td>BEBIDAS CARBONATADAS</td>\n",
       "      <td>GASEOSAS</td>\n",
       "      <td>PREMIUM</td>\n",
       "      <td>BOTELLA</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>57290</td>\n",
       "      <td>Brand 31</td>\n",
       "      <td>BEBIDAS CARBONATADAS</td>\n",
       "      <td>GASEOSAS</td>\n",
       "      <td>PREMIUM</td>\n",
       "      <td>BOTELLA</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56714</td>\n",
       "      <td>Brand 31</td>\n",
       "      <td>BEBIDAS CARBONATADAS</td>\n",
       "      <td>GASEOSAS</td>\n",
       "      <td>PREMIUM</td>\n",
       "      <td>BOTELLA</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>296616</td>\n",
       "      <td>Brand 31</td>\n",
       "      <td>BEBIDAS CARBONATADAS</td>\n",
       "      <td>GASEOSAS</td>\n",
       "      <td>PREMIUM</td>\n",
       "      <td>BOTELLA</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60854</td>\n",
       "      <td>Brand 31</td>\n",
       "      <td>BEBIDAS CARBONATADAS</td>\n",
       "      <td>GASEOSAS</td>\n",
       "      <td>PREMIUM</td>\n",
       "      <td>BOTELLA</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_id     brand              category sub_category  segment  package  \\\n",
       "0       34092  Brand 31  BEBIDAS CARBONATADAS     GASEOSAS  PREMIUM  BOTELLA   \n",
       "1       57290  Brand 31  BEBIDAS CARBONATADAS     GASEOSAS  PREMIUM  BOTELLA   \n",
       "2       56714  Brand 31  BEBIDAS CARBONATADAS     GASEOSAS  PREMIUM  BOTELLA   \n",
       "3      296616  Brand 31  BEBIDAS CARBONATADAS     GASEOSAS  PREMIUM  BOTELLA   \n",
       "4       60854  Brand 31  BEBIDAS CARBONATADAS     GASEOSAS  PREMIUM  BOTELLA   \n",
       "\n",
       "   size  \n",
       "0  0.66  \n",
       "1  0.66  \n",
       "2  0.66  \n",
       "3  0.66  \n",
       "4  0.25  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_products.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 971 entries, 0 to 970\n",
      "Data columns (total 7 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   product_id    971 non-null    int64  \n",
      " 1   brand         971 non-null    object \n",
      " 2   category      971 non-null    object \n",
      " 3   sub_category  971 non-null    object \n",
      " 4   segment       971 non-null    object \n",
      " 5   package       971 non-null    object \n",
      " 6   size          971 non-null    float64\n",
      "dtypes: float64(1), int64(1), object(5)\n",
      "memory usage: 53.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df_products.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>order_id</th>\n",
       "      <th>purchase_date</th>\n",
       "      <th>items</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>61353</td>\n",
       "      <td>61364</td>\n",
       "      <td>411145</td>\n",
       "      <td>2024-04-27</td>\n",
       "      <td>-0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>61899</td>\n",
       "      <td>1370</td>\n",
       "      <td>411156</td>\n",
       "      <td>2024-04-27</td>\n",
       "      <td>3.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>154077</td>\n",
       "      <td>30500</td>\n",
       "      <td>417911</td>\n",
       "      <td>2024-04-29</td>\n",
       "      <td>2.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>164337</td>\n",
       "      <td>56714</td>\n",
       "      <td>418100</td>\n",
       "      <td>2024-04-29</td>\n",
       "      <td>-2.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>172056</td>\n",
       "      <td>61672</td>\n",
       "      <td>407162</td>\n",
       "      <td>2024-04-26</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     customer_id  product_id  order_id purchase_date     items\n",
       "124        61353       61364    411145    2024-04-27 -0.333333\n",
       "127        61899        1370    411156    2024-04-27  3.666667\n",
       "344       154077       30500    417911    2024-04-29  2.333333\n",
       "349       164337       56714    418100    2024-04-29 -2.333333\n",
       "354       172056       61672    407162    2024-04-26  1.000000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_transactions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 254936 entries, 124 to 7712548\n",
      "Data columns (total 5 columns):\n",
      " #   Column         Non-Null Count   Dtype         \n",
      "---  ------         --------------   -----         \n",
      " 0   customer_id    254936 non-null  int64         \n",
      " 1   product_id     254936 non-null  int64         \n",
      " 2   order_id       254936 non-null  int64         \n",
      " 3   purchase_date  254936 non-null  datetime64[ns]\n",
      " 4   items          254936 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(1), int64(3)\n",
      "memory usage: 11.7 MB\n"
     ]
    }
   ],
   "source": [
    "df_transactions.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir del an√°lisis de los dataframes, podemos concluir que los datos de clientes, productos y transacciones tienen correctamente asignados sus tipos de datos. Adem√°s, solo se detect√≥ un valor nulo, ubicado en la variable \"X\" (longitud) dentro de los datos de clientes.\n",
    "\n",
    "Con esta breve exploraci√≥n se determina que, para resolver el problema, ser√° necesario realizar un cruce de los tres dataframes. El procedimiento ser√° el siguiente: en primer lugar, se generar√° en el dataframe de transacciones una variable que indique la semana correspondiente a cada compra. Posteriormente, se agrupar√°n los registros por cliente, producto y semana, y a este resultado se le aplicar√° un join con las otras dos fuentes de datos.\n",
    "\n",
    "Finalmente, se construir√° la variable objetivo \"bought\", asign√°ndole el valor 1, ya que su presencia en los datos implica la existencia de una combinaci√≥n v√°lida entre cliente, producto y semana. Sin embargo, esta metodolog√≠a solo generar√≠a etiquetas positivas; por ello, se crear√°n todas la combinaciones posibles de clientes, productos y semanas que no aparezcan en los datos originales, asign√°ndoles a la variable objetivo el valor 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Craci√≥n variable week\n",
    "Se crea la variable week y se agrupa por esta. Se va agregar la variable purchase_count que cuenta la cantidad de veces que un cliente compro un producto en una semana determinada. Asimismo, para itemes (cantidad de bultos comprados) se calcular√° el promedio. Finalmente la variable order_id es descartada, no aporta nada al problema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 206030 entries, 0 to 206029\n",
      "Data columns (total 5 columns):\n",
      " #   Column          Non-Null Count   Dtype  \n",
      "---  ------          --------------   -----  \n",
      " 0   customer_id     206030 non-null  object \n",
      " 1   product_id      206030 non-null  object \n",
      " 2   week            206030 non-null  UInt32 \n",
      " 3   purchase_count  206030 non-null  int64  \n",
      " 4   average_items   206030 non-null  float64\n",
      "dtypes: UInt32(1), float64(1), int64(1), object(2)\n",
      "memory usage: 7.3+ MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>week</th>\n",
       "      <th>purchase_count</th>\n",
       "      <th>average_items</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>123150</td>\n",
       "      <td>10652</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>123150</td>\n",
       "      <td>11262</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>123150</td>\n",
       "      <td>11262</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>2.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>123150</td>\n",
       "      <td>11262</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>2.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>123150</td>\n",
       "      <td>11262</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>1.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  customer_id product_id  week  purchase_count  average_items\n",
       "0      123150      10652     9               1       1.666667\n",
       "1      123150      11262     2               1       2.333333\n",
       "2      123150      11262     9               1       2.333333\n",
       "3      123150      11262    31               1       2.333333\n",
       "4      123150      11262    39               1       1.666667"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transformar id's a string's\n",
    "df_customers[\"customer_id\"] = df_customers[\"customer_id\"].astype(str)\n",
    "df_products[\"product_id\"] = df_products[\"product_id\"].astype(str)\n",
    "df_transactions[\"customer_id\"] = df_transactions[\"customer_id\"].astype(str)\n",
    "df_transactions[\"product_id\"] = df_transactions[\"product_id\"].astype(str)\n",
    "df_transactions[\"order_id\"] = df_transactions[\"order_id\"].astype(str)\n",
    "\n",
    "# Crear variable week\n",
    "df_transactions[\"week\"] = df_transactions[\"purchase_date\"].dt.isocalendar().week\n",
    "\n",
    "# Agrupar por customer_id, product_id y week, contando las compras y promediando los items\n",
    "df_gruped = df_transactions.groupby([\"customer_id\", \"product_id\", \"week\"]).agg(\n",
    "    purchase_count=('order_id', 'count'),\n",
    "    average_items=('items', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "print(df_gruped.info())\n",
    "df_gruped.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cruce de informaci√≥n\n",
    "Se hace el join de las compras con los clientes y productos, y se crea la variable objetivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 206030 entries, 0 to 206029\n",
      "Data columns (total 19 columns):\n",
      " #   Column                Non-Null Count   Dtype  \n",
      "---  ------                --------------   -----  \n",
      " 0   customer_id           206030 non-null  object \n",
      " 1   product_id            206030 non-null  object \n",
      " 2   week                  206030 non-null  UInt32 \n",
      " 3   purchase_count        206030 non-null  int64  \n",
      " 4   average_items         206030 non-null  float64\n",
      " 5   region_id             206030 non-null  int64  \n",
      " 6   zone_id               206030 non-null  int64  \n",
      " 7   customer_type         206030 non-null  object \n",
      " 8   Y                     206030 non-null  float64\n",
      " 9   X                     206030 non-null  float64\n",
      " 10  num_deliver_per_week  206030 non-null  int64  \n",
      " 11  num_visit_per_week    206030 non-null  int64  \n",
      " 12  brand                 206030 non-null  object \n",
      " 13  category              206030 non-null  object \n",
      " 14  sub_category          206030 non-null  object \n",
      " 15  segment               206030 non-null  object \n",
      " 16  package               206030 non-null  object \n",
      " 17  size                  206030 non-null  float64\n",
      " 18  bought                206030 non-null  int8   \n",
      "dtypes: UInt32(1), float64(4), int64(5), int8(1), object(8)\n",
      "memory usage: 27.9+ MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>week</th>\n",
       "      <th>purchase_count</th>\n",
       "      <th>average_items</th>\n",
       "      <th>region_id</th>\n",
       "      <th>zone_id</th>\n",
       "      <th>customer_type</th>\n",
       "      <th>Y</th>\n",
       "      <th>X</th>\n",
       "      <th>num_deliver_per_week</th>\n",
       "      <th>num_visit_per_week</th>\n",
       "      <th>brand</th>\n",
       "      <th>category</th>\n",
       "      <th>sub_category</th>\n",
       "      <th>segment</th>\n",
       "      <th>package</th>\n",
       "      <th>size</th>\n",
       "      <th>bought</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>123150</td>\n",
       "      <td>10652</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>80</td>\n",
       "      <td>5148</td>\n",
       "      <td>ABARROTES</td>\n",
       "      <td>-46.69453</td>\n",
       "      <td>-107.832138</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Brand 34</td>\n",
       "      <td>BEBIDAS CARBONATADAS</td>\n",
       "      <td>GASEOSAS</td>\n",
       "      <td>MEDIUM</td>\n",
       "      <td>LATA</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>123150</td>\n",
       "      <td>11262</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>80</td>\n",
       "      <td>5148</td>\n",
       "      <td>ABARROTES</td>\n",
       "      <td>-46.69453</td>\n",
       "      <td>-107.832138</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Brand 24</td>\n",
       "      <td>BEBIDAS CARBONATADAS</td>\n",
       "      <td>GASEOSAS</td>\n",
       "      <td>LOW</td>\n",
       "      <td>LATA</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>123150</td>\n",
       "      <td>11262</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>80</td>\n",
       "      <td>5148</td>\n",
       "      <td>ABARROTES</td>\n",
       "      <td>-46.69453</td>\n",
       "      <td>-107.832138</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Brand 24</td>\n",
       "      <td>BEBIDAS CARBONATADAS</td>\n",
       "      <td>GASEOSAS</td>\n",
       "      <td>LOW</td>\n",
       "      <td>LATA</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>123150</td>\n",
       "      <td>11262</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>80</td>\n",
       "      <td>5148</td>\n",
       "      <td>ABARROTES</td>\n",
       "      <td>-46.69453</td>\n",
       "      <td>-107.832138</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Brand 24</td>\n",
       "      <td>BEBIDAS CARBONATADAS</td>\n",
       "      <td>GASEOSAS</td>\n",
       "      <td>LOW</td>\n",
       "      <td>LATA</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>123150</td>\n",
       "      <td>11262</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>80</td>\n",
       "      <td>5148</td>\n",
       "      <td>ABARROTES</td>\n",
       "      <td>-46.69453</td>\n",
       "      <td>-107.832138</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Brand 24</td>\n",
       "      <td>BEBIDAS CARBONATADAS</td>\n",
       "      <td>GASEOSAS</td>\n",
       "      <td>LOW</td>\n",
       "      <td>LATA</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  customer_id product_id  week  purchase_count  average_items  region_id  \\\n",
       "0      123150      10652     9               1       1.666667         80   \n",
       "1      123150      11262     2               1       2.333333         80   \n",
       "2      123150      11262     9               1       2.333333         80   \n",
       "3      123150      11262    31               1       2.333333         80   \n",
       "4      123150      11262    39               1       1.666667         80   \n",
       "\n",
       "   zone_id customer_type         Y           X  num_deliver_per_week  \\\n",
       "0     5148     ABARROTES -46.69453 -107.832138                     4   \n",
       "1     5148     ABARROTES -46.69453 -107.832138                     4   \n",
       "2     5148     ABARROTES -46.69453 -107.832138                     4   \n",
       "3     5148     ABARROTES -46.69453 -107.832138                     4   \n",
       "4     5148     ABARROTES -46.69453 -107.832138                     4   \n",
       "\n",
       "   num_visit_per_week     brand              category sub_category segment  \\\n",
       "0                   1  Brand 34  BEBIDAS CARBONATADAS     GASEOSAS  MEDIUM   \n",
       "1                   1  Brand 24  BEBIDAS CARBONATADAS     GASEOSAS     LOW   \n",
       "2                   1  Brand 24  BEBIDAS CARBONATADAS     GASEOSAS     LOW   \n",
       "3                   1  Brand 24  BEBIDAS CARBONATADAS     GASEOSAS     LOW   \n",
       "4                   1  Brand 24  BEBIDAS CARBONATADAS     GASEOSAS     LOW   \n",
       "\n",
       "  package  size  bought  \n",
       "0    LATA  0.25       1  \n",
       "1    LATA  0.25       1  \n",
       "2    LATA  0.25       1  \n",
       "3    LATA  0.25       1  \n",
       "4    LATA  0.25       1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Union de las tablas\n",
    "df_merged = df_gruped.merge(df_customers, on='customer_id', how='inner')\n",
    "df_merged = df_merged.merge(df_products, on='product_id', how='inner')\n",
    "\n",
    "# Crear variable objetivo\n",
    "df_merged['bought'] = 1\n",
    "df_merged['bought'] = df_merged['bought'].astype('int8')\n",
    "print(df_merged.info())\n",
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creaci√≥n variables objetivos negativas (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformaci√≥n de tipo para ahorrar memoria\n",
    "\n",
    "def optimize_numeric_types(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convierte de una sola vez:\n",
    "    - int64  -> int32\n",
    "    - float64 -> float32\n",
    "    Mantiene el resto igual.\n",
    "    \"\"\"\n",
    "    dtypes = df.dtypes\n",
    "    mapping = {}\n",
    "\n",
    "    # Selecci√≥n vectorizada con numpy para evitar for\n",
    "    mask_int64   = dtypes.values == np.dtype(\"int64\")\n",
    "    mask_float64 = dtypes.values == np.dtype(\"float64\")\n",
    "\n",
    "    if mask_int64.any():\n",
    "        mapping.update(dict(zip(dtypes.index[mask_int64], np.repeat(\"int32\", mask_int64.sum()))))\n",
    "    if mask_float64.any():\n",
    "        mapping.update(dict(zip(dtypes.index[mask_float64], np.repeat(\"float32\", mask_float64.sum()))))\n",
    "\n",
    "    return df.astype(mapping, copy=False)\n",
    "\n",
    "\n",
    "# IDs como category\n",
    "for frame, cols in [\n",
    "    (df_customers, [\"customer_id\"]),\n",
    "    (df_products,  [\"product_id\"]),\n",
    "    (df_transactions, [\"customer_id\", \"product_id\"]),\n",
    "    (df_merged,   [\"customer_id\", \"product_id\"]),\n",
    "]:\n",
    "    frame[cols] = frame[cols].astype(\"category\")\n",
    "\n",
    "# Optimizaci√≥n num√©rica \n",
    "df_transactions = optimize_numeric_types(df_transactions)\n",
    "df_merged       = optimize_numeric_types(df_merged)\n",
    "df_customers    = optimize_numeric_types(df_customers)\n",
    "df_products     = optimize_numeric_types(df_products)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 151. MiB for an array with shape (79221948,) and data type int16",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      7\u001b[39m weeks = weeks.astype(\u001b[33m\"\u001b[39m\u001b[33mint16\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Todos los clientes √ó productos √ó semanas\u001b[39;00m\n\u001b[32m     10\u001b[39m universe = (\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mMultiIndex\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_product\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mdf_customers\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcustomer_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcategories\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m         \u001b[49m\u001b[43mdf_products\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mproduct_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcategories\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m         \u001b[49m\u001b[43mweeks\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcustomer_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mproduct_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweek\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m     .to_frame(index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     18\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shabe\\miniforge3\\envs\\MDS-lab\\Lib\\site-packages\\pandas\\core\\indexes\\multi.py:685\u001b[39m, in \u001b[36mMultiIndex.from_product\u001b[39m\u001b[34m(cls, iterables, sortorder, names)\u001b[39m\n\u001b[32m    682\u001b[39m     names = [\u001b[38;5;28mgetattr\u001b[39m(it, \u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m it \u001b[38;5;129;01min\u001b[39;00m iterables]\n\u001b[32m    684\u001b[39m \u001b[38;5;66;03m# codes are all ndarrays, so cartesian_product is lossless\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m685\u001b[39m codes = \u001b[43mcartesian_product\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    686\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(levels, codes, sortorder=sortorder, names=names)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shabe\\miniforge3\\envs\\MDS-lab\\Lib\\site-packages\\pandas\\core\\reshape\\util.py:64\u001b[39m, in \u001b[36mcartesian_product\u001b[39m\u001b[34m(X)\u001b[39m\n\u001b[32m     59\u001b[39m     b = np.zeros_like(cumprodX)\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# error: Argument of type \"int_\" cannot be assigned to parameter \"num\" of\u001b[39;00m\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# type \"int\" in function \"tile_compat\"\u001b[39;00m\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m     \u001b[43mtile_compat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrepeat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprod\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(X)\n\u001b[32m     69\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shabe\\miniforge3\\envs\\MDS-lab\\Lib\\site-packages\\pandas\\core\\reshape\\util.py:81\u001b[39m, in \u001b[36mtile_compat\u001b[39m\u001b[34m(arr, num)\u001b[39m\n\u001b[32m     73\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     74\u001b[39m \u001b[33;03mIndex compat for np.tile.\u001b[39;00m\n\u001b[32m     75\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     78\u001b[39m \u001b[33;03mDoes not support multi-dimensional `num`.\u001b[39;00m\n\u001b[32m     79\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, np.ndarray):\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtile\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[38;5;66;03m# Otherwise we have an Index\u001b[39;00m\n\u001b[32m     84\u001b[39m taker = np.tile(np.arange(\u001b[38;5;28mlen\u001b[39m(arr)), num)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shabe\\miniforge3\\envs\\MDS-lab\\Lib\\site-packages\\numpy\\lib\\_shape_base_impl.py:1287\u001b[39m, in \u001b[36mtile\u001b[39m\u001b[34m(A, reps)\u001b[39m\n\u001b[32m   1283\u001b[39m d = \u001b[38;5;28mlen\u001b[39m(tup)\n\u001b[32m   1284\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(x == \u001b[32m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m tup) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(A, _nx.ndarray):\n\u001b[32m   1285\u001b[39m     \u001b[38;5;66;03m# Fixes the problem that the function does not make a copy if A is a\u001b[39;00m\n\u001b[32m   1286\u001b[39m     \u001b[38;5;66;03m# numpy array and the repetitions are 1 in all dimensions\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1287\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubok\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndmin\u001b[49m\u001b[43m=\u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1288\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1289\u001b[39m     \u001b[38;5;66;03m# Note that no copy of zero-sized arrays is made. However since they\u001b[39;00m\n\u001b[32m   1290\u001b[39m     \u001b[38;5;66;03m# have no data there is no risk of an inadvertent overwrite.\u001b[39;00m\n\u001b[32m   1291\u001b[39m     c = _nx.array(A, copy=\u001b[38;5;28;01mNone\u001b[39;00m, subok=\u001b[38;5;28;01mTrue\u001b[39;00m, ndmin=d)\n",
      "\u001b[31mMemoryError\u001b[39m: Unable to allocate 151. MiB for an array with shape (79221948,) and data type int16"
     ]
    }
   ],
   "source": [
    "# Crear universo cliente-producto-semana\n",
    "# Fechas m√≠n/m√°x para generar semanas completas\n",
    "min_week, max_week = df_transactions[\"week\"].min(), df_transactions[\"week\"].max()\n",
    "weeks = pd.Series(df_transactions[\"week\"].unique(), name=\"week\")\n",
    "\n",
    "# Transformar semanas a int16 para ahorrar memoria\n",
    "weeks = weeks.astype(\"int16\")\n",
    "\n",
    "# Todos los clientes √ó productos √ó semanas\n",
    "universe = (\n",
    "    pd.MultiIndex.from_product(\n",
    "        [df_customers[\"customer_id\"].cat.categories,\n",
    "         df_products[\"product_id\"].cat.categories,\n",
    "         weeks],\n",
    "        names=[\"customer_id\", \"product_id\", \"week\"]\n",
    "    )\n",
    "    .to_frame(index=False)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Merge para target binario\n",
    "data = universe.merge(df_merged, on=[\"customer_id\", \"product_id\", \"week\"], how=\"left\")\n",
    "data[\"bought\"] = data[\"bought\"].fillna(0).astype(\"int8\")\n",
    "\n",
    "# Agregar features de cliente y producto\n",
    "data = data.merge(df_customers, on=\"customer_id\", how=\"left\")\n",
    "data = data.merge(df_products, on=\"product_id\", how=\"left\")\n",
    "\n",
    "# Eliminar duplicados\n",
    "data = data.drop_duplicates()\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìå EDA [0.5 puntos]\n",
    "\n",
    "<center>\n",
    "<img src=\"https://media3.giphy.com/media/v1.Y2lkPTc5MGI3NjExbHZ6aGdkd21tYTI3cW8zYWhyYW5wdGlyb2s3MmRzeTV0dzQ1NWlueiZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/3k1hJubTtOAKPKx4k3/giphy.gif\" width=\"400\" height=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta secci√≥n, se debe realizar un an√°lisis exploratorio de los datos para comprender su estructura, detectar posibles problemas y obtener informaci√≥n relevante para el entrenamiento de los modelos. La idea es que puedan detectar **patrones en los datos** que les permitan resolver el problema con mayor facilidad.\n",
    "\n",
    "Se deben responder preguntas a partir de lo que puedan visualizar/obtener, por ejemplo:\n",
    "\n",
    "- Clientes y productos\n",
    "\n",
    "    - ¬øCu√°ntos clientes √∫nicos hay en el dataset?\n",
    "\n",
    "    - ¬øCu√°ntos productos √∫nicos se encuentran en los datos?\n",
    "\n",
    "- Periodo y frecuencia\n",
    "\n",
    "    - ¬øDe qu√© periodo es la informaci√≥n disponible?\n",
    "\n",
    "    - ¬øCu√°l es la frecuencia de los registros (diaria, semanal, mensual, etc.)?\n",
    "\n",
    "- Calidad de los datos\n",
    "\n",
    "    - ¬øExisten valores nulos en el dataset? ¬øCu√°ntos? ¬øC√≥mo se pueden tratar?\n",
    "\n",
    "    - ¬øHay datos raros, como cantidades negativas o inconsistencias? Genere tests de validaci√≥n para identificar estos problemas.\n",
    "\n",
    "- Patrones de compra\n",
    "\n",
    "    - ¬øCu√°ntos productos compra en promedio cada cliente semana a semana?\n",
    "\n",
    "    - ¬øCu√°ntas transacciones ha realizado cada cliente?\n",
    "\n",
    "    - ¬øCu√°l es el periodo de recompra promedio de cada SKU?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df = df_final.copy()  # viene del pipeline de la secci√≥n anterior\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Clientes √∫nicos\n",
    "num_customers = df['customer_id'].nunique()\n",
    "print(f\"Clientes √∫nicos: {num_customers}\")\n",
    "\n",
    "# Clientes que nunca compraron\n",
    "all_customers = set(df_customers['customer_id'])\n",
    "purchasing_customers = set(df['customer_id'])\n",
    "never_purchased_customers = all_customers - purchasing_customers\n",
    "print(f\"Clientes que nunca compraron: {len(never_purchased_customers)}\")\n",
    "\n",
    "# Productos √∫nicos\n",
    "num_products = df['product_id'].nunique()\n",
    "print(f\"Productos √∫nicos: {num_products}\")\n",
    "\n",
    "# Productos que nunca fueron comprados\n",
    "all_products = set(df_products['product_id'])\n",
    "purchased_products = set(df['product_id'])\n",
    "never_purchased = all_products - purchased_products\n",
    "print(f\"Productos que nunca fueron comprados: {len(never_purchased)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay 79 clientes que nunca compraron y 857 productos que no aparecen en las transacciones por lo que fueron descartados. Estos fueron descartados al hacer el merge \"inner\" y no el \"outer\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PERIODO Y FRECUENCIA\n",
    "\n",
    "min_date = df['purchase_date'].min()\n",
    "max_date = df['purchase_date'].max()\n",
    "print(f\"Periodo de datos: {min_date} ‚Üí {max_date}\")\n",
    "# Chequear frecuencia promedio de las transacciones totales\n",
    "diffs = df['purchase_date'].sort_values().diff().dropna()\n",
    "print(f\"\\nFrecuencia m√°s com√∫n entre registros: {diffs.mode()[0]}\")\n",
    "print(f\"Frecuencia promedio entre registros: {diffs.mean()}\")\n",
    "print(f\"Frecuencia m√≠nima entre registros: {diffs.min()}\")\n",
    "print(f\"Frecuencia m√°xima entre registros: {diffs.max()}\")\n",
    "\n",
    "# Chequear frecuencia promedio por cliente\n",
    "df = df.sort_values(by=['customer_id', 'purchase_date'])\n",
    "customer_diffs = df.groupby('customer_id')['purchase_date'].diff().dropna()\n",
    "print(f\"\\nFrecuencia m√°s com√∫n entre registros por cliente: {customer_diffs.mode()[0]}\")\n",
    "print(f\"Frecuencia promedio entre registros por cliente: {customer_diffs.mean()}\")\n",
    "print(f\"Frecuencia m√≠nima entre registros por cliente: {customer_diffs.min()}\")\n",
    "print(f\"Frecuencia m√°xima entre registros por cliente: {customer_diffs.max()}\")\n",
    "\n",
    "# Conteo de transacciones por fecha\n",
    "tx_per_day = df.groupby('purchase_date')['order_id'].count()\n",
    "tx_per_day.plot(figsize=(12,5), title=\"Transacciones por d√≠a\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CALIDAD DE LOS DATOS\n",
    "# Nulos\n",
    "print(\"Valores nulos por columna:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Detecci√≥n de problemas\n",
    "def check_data_quality(df):\n",
    "    issues = {}\n",
    "    if (df['items'] < 0).any():\n",
    "        issues['negative_items'] = df[(df['items'] < 0)].shape[0]\n",
    "    if (df[\"items\"] % 1).any():\n",
    "        issues['non_integer_items'] = df[~df[\"items\"].apply(lambda x: x == int(x) if pd.notna(x) else True)].shape[0]\n",
    "    if (df[\"items\"] == 0).any():\n",
    "        issues['zero_items'] = df[df[\"items\"] == 0].shape[0]\n",
    "    if (df['size'] <= 0).any():\n",
    "        issues['invalid_size'] = df[df['size'] <= 0].shape[0]\n",
    "    if (df[\"num_deliver_per_week\"] < 0).any():\n",
    "        issues['negative_delivery_frequency'] = df[df[\"num_deliver_per_week\"] < 0].shape[0]\n",
    "    if (df[\"num_visit_per_week\"] < 0).any():\n",
    "        issues['negative_visit_frequency'] = df[df[\"num_visit_per_week\"] < 0].shape[0]\n",
    "    return issues\n",
    "print(\"\\nProblemas detectados:\")\n",
    "print(check_data_quality(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((df_transactions[\"items\"][(df_transactions[\"items\"] % 1) != 0] % 1).value_counts())\n",
    "\n",
    "plt.hist(df[\"items\"], bins=30, color='blue', alpha=0.7)\n",
    "plt.title(\"Distribuci√≥n de 'items'\")\n",
    "plt.xlabel(\"N√∫mero de items\")\n",
    "plt.ylabel(\"Frecuencia\")\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO Justificar multiplo de 3\n",
    "Se puede observar que existen valores nulos en todas las columnas pero no superan el 0.36% de los datos. Para corregir esto utilizaremos un Imputer porque ya eliminamos las filas que contenian solo valores nulos. Adem√°s, se observa existen 8346 datos negativos, 146451 datos decimales y 1 dato igual a 0 en la columan de items. Ni uno de estos datos tiene mucho sentido cuando hablamos sobre la cantidad de bultos que se compraron en un transacci√≥n. Para los decimales vamos a asumir que items no se refiere a una unidad sino a un pack de productos por eso es que existen decimales. En cuanto a los valores negativos los convertiremos a positivos ya que creemos que es un error al momento de ingresar los datos, en el mundo real lo que har√≠amos ser√≠a revisar la forma en la que se ingresan los datos y a partir de eso se tomar√≠a la decisi√≥n de convertirlos a positivos o eliminarlos. Por √∫ltimo, la fila que contiene una transacci√≥n con cero items ser√° eliminada, no tiene sentido hacer una compra por cero productos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Correci√≥n de problemas en items\n",
    "df_copy = df.copy()\n",
    "# multiplicar\n",
    "df_copy[\"items_times3\"] = df_copy[\"items\"] * 3  \n",
    "\n",
    "# tolerancia para decidir si es entero (ej: 1e-6)\n",
    "tolerance = 1e-6\n",
    "mask = np.isclose(df_copy[\"items_times3\"], df_copy[\"items_times3\"].round(), atol=tolerance)\n",
    "\n",
    "# quedarnos solo con los m√∫ltiplos de 1/3\n",
    "df_copy = df_copy[mask].copy()\n",
    "\n",
    "# finalmente redondeamos y pasamos a int\n",
    "df_copy[\"items\"] = df_copy[\"items_times3\"].round().astype(int)\n",
    "\n",
    "# eliminamos la columna auxiliar\n",
    "df_copy = df_copy.drop(columns=\"items_times3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patrones de compra\n",
    "# Productos comprados promedio por cliente por semana\n",
    "df['week'] = df['purchase_date'].dt.to_period('W')\n",
    "prod_per_client_week = (\n",
    "    df.groupby(['customer_id','week'])['product_id']\n",
    "    .nunique()\n",
    ")\n",
    "print(\"Distribuci√≥n de productos comprados promedio por cliente por semana:\")\n",
    "print(prod_per_client_week.describe())\n",
    "\n",
    "sns.histplot(prod_per_client_week, bins=50)\n",
    "plt.title(\"Distribuci√≥n de productos distintos comprados por cliente/semana\")\n",
    "plt.show()\n",
    "\n",
    "# Transacciones por cliente\n",
    "tx_per_client = df.groupby('customer_id')['order_id'].nunique()\n",
    "print(\"Distribuci√≥n de transacciones por cliente:\")\n",
    "print(tx_per_client.describe())\n",
    "\n",
    "sns.histplot(tx_per_client, bins=50)\n",
    "plt.title(\"Distribuci√≥n de transacciones por cliente\")\n",
    "plt.show()\n",
    "\n",
    "# Periodo de recompra promedio por SKU\n",
    "df_sorted = df.sort_values(['product_id','purchase_date'])\n",
    "df_sorted['prev_purchase'] = df_sorted.groupby('product_id')['purchase_date'].shift(1)\n",
    "df_sorted['recompra_dias'] = (df_sorted['purchase_date'] - df_sorted['prev_purchase']).dt.days\n",
    "\n",
    "recompra_promedio = df_sorted.groupby('product_id')['recompra_dias'].mean().dropna()\n",
    "print(\"Recompra promedio (en d√≠as) por SKU:\")\n",
    "print(recompra_promedio.describe())\n",
    "\n",
    "sns.histplot(recompra_promedio, bins=50)\n",
    "plt.title(\"Distribuci√≥n de d√≠as promedio entre compras por SKU\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìå Holdout [0.25 puntos]\n",
    "\n",
    "Para evaluar correctamente el modelo y garantizar su capacidad de generalizaci√≥n, se deben dividir los datos en tres conjuntos: \n",
    "- `Entrenamiento` : Para ajustar los par√°metros.\n",
    "- `Validaci√≥n`: Para optimizar hiperpar√°metros y seleccionar el mejor modelo.\n",
    "- `Prueba` : Para evaluar el rendimiento final en datos no vistos.\n",
    "\n",
    "üëÄ **Hint**: *Recuerde que los datos tienen una temporalidad que debe considerarse al momento de separarlos, para evitar fugas de informaci√≥n. Es importante justificar la estrategia de partici√≥n elegida y visualizar la distribuci√≥n temporal de los conjuntos generados*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Ordenar datos por fecha para respetar temporalidad\n",
    "df_temporal = df_final.sort_values('purchase_date').copy()\n",
    "\n",
    "# Definir puntos de corte temporal\n",
    "# 70% train, 15% validation, 15% test\n",
    "train_end = df_temporal['purchase_date'].quantile(0.70)\n",
    "val_end = df_temporal['purchase_date'].quantile(0.85)\n",
    "\n",
    "# Separar los conjuntos basados en tiempo\n",
    "train_data = df_temporal[df_temporal['purchase_date'] <= train_end]\n",
    "val_data = df_temporal[(df_temporal['purchase_date'] > train_end) & \n",
    "                       (df_temporal['purchase_date'] <= val_end)]\n",
    "test_data = df_temporal[df_temporal['purchase_date'] > val_end]\n",
    "\n",
    "print(\"Distribuci√≥n temporal de los conjuntos:\")\n",
    "print(f\"Train: {train_data['purchase_date'].min()} ‚Üí {train_data['purchase_date'].max()} ({len(train_data)} registros)\")\n",
    "print(f\"Val:   {val_data['purchase_date'].min()} ‚Üí {val_data['purchase_date'].max()} ({len(val_data)} registros)\")\n",
    "print(f\"Test:  {test_data['purchase_date'].min()} ‚Üí {test_data['purchase_date'].max()} ({len(test_data)} registros)\")\n",
    "\n",
    "# Visualizar la distribuci√≥n temporal\n",
    "fig, axes = plt.subplots(3, 1, figsize=(12, 8))\n",
    "\n",
    "axes[0].hist(train_data['purchase_date'], bins=50, alpha=0.7, color='blue')\n",
    "axes[0].set_title('Conjunto de Entrenamiento')\n",
    "axes[0].set_ylabel('Frecuencia')\n",
    "\n",
    "axes[1].hist(val_data['purchase_date'], bins=30, alpha=0.7, color='orange')\n",
    "axes[1].set_title('Conjunto de Validaci√≥n')\n",
    "axes[1].set_ylabel('Frecuencia')\n",
    "\n",
    "axes[2].hist(test_data['purchase_date'], bins=30, alpha=0.7, color='green')\n",
    "axes[2].set_title('Conjunto de Prueba')\n",
    "axes[2].set_ylabel('Frecuencia')\n",
    "axes[2].set_xlabel('Fecha de compra')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Crear target variable: 1 si el cliente compr√≥ el producto, 0 si no\n",
    "# Para esto necesitamos crear todas las combinaciones posibles cliente-producto\n",
    "# y marcar cu√°les efectivamente ocurrieron\n",
    "\n",
    "# Obtener fechas √∫nicas por conjunto\n",
    "train_dates = train_data['purchase_date'].unique()\n",
    "val_dates = val_data['purchase_date'].unique()\n",
    "test_dates = test_data['purchase_date'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìå Feature Engineering [0.5 puntos]\n",
    "\n",
    "<center>\n",
    "<img src=\"https://i.imgur.com/CmXZSSC.gif\" width=\"300\" height=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta secci√≥n, se deben construir pipelines para automatizar el preprocesamiento de los datos, lo cual garantizar√° que el flujo de trabajo sea reproducible y eficiente para esta entrega y las futuras. El objetivo es aplicar una serie de transformaciones en un orden definido para asegurar que los datos est√©n listos para los modelos a entrenar. El pipeline final debe incluir las t√©cnicas de pre-procesamiento que se deben aplicar a los distintos datos (seg√∫n lo que consideren necesario para el problema). Por ejemplo:\n",
    "\n",
    "- **Imputaci√≥n de valores nulos**: Manejo de datos faltantes mediante estrategias adecuadas (media, mediana, moda, interpolaci√≥n, etc.). \n",
    "\n",
    "- **Transformaciones personalizadas**: Uso de ColumnTransformer para aplicar diferentes transformaciones a columnas espec√≠ficas.\n",
    "\n",
    "- **Codificaci√≥n de variables categ√≥ricas**: Convertir datos categ√≥ricos a un formato num√©rico adecuado (One-Hot Encoding, Label Encoding, etc.).\n",
    "\n",
    "- **Discretizaci√≥n de variables**: Conversi√≥n de variables num√©ricas continuas en categor√≠as si son relevantes para el desempe√±o del modelo a entrenar.\n",
    "\n",
    "- **Estandarizaci√≥n o normalizaci√≥n** : Ajustar la escala de los datos para mejorar el rendimiento de los algoritmos sensibles a la magnitud de las variables.\n",
    "\n",
    "- **Eliminaci√≥n o transformaci√≥n de valores at√≠picos**: Identificar y tratar con datos outliers para mejorar la robustez del modelo.\n",
    "\n",
    "- **Nuevas caracter√≠sticas** : Creaci√≥n de variables adicionales que puedan aportar informaci√≥n relevante al modelo.\n",
    "\n",
    "Cada una de estas transformaciones debe ser justificada en funci√≥n de su relevancia para el problema y los datos, y es importante evaluar su impacto en el rendimiento del modelo. Adem√°s, el pipeline debe ser flexible y modular para poder probar diferentes configuraciones de preprocesamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "df_pipe = df_final.copy()  # viene del pipeline de la secci√≥n anterior\n",
    "\n",
    "numerical_features = df_pipe.select_dtypes(include=['int32', 'int64', 'float32', 'float64']).columns.tolist()\n",
    "categorical_features = df_pipe.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(\"Columnas num√©ricas:\", numerical_features)\n",
    "print(\"Columnas categ√≥ricas:\", categorical_features)\n",
    "\n",
    "\n",
    "# Pipeline para features num√©ricas\n",
    "numerical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),  # Imputar valores faltantes con mediana\n",
    "    ('scaler', StandardScaler())  # Estandarizar valores\n",
    "])\n",
    "\n",
    "# Pipeline para features categ√≥ricas\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),  # Imputar con 'missing'\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))  # One-hot encoding\n",
    "])\n",
    "\n",
    "# Combinar pipelines con ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_pipeline, numerical_features),\n",
    "        ('cat', categorical_pipeline, categorical_features)\n",
    "    ],\n",
    "    remainder='drop'  # Descartar columnas no especificadas\n",
    ")\n",
    "\n",
    "# Crear pipeline completo de preprocesamiento\n",
    "feature_engineering_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor)\n",
    "])\n",
    "\n",
    "# Aplicar transformaciones a los conjuntos\n",
    "print(\"\\nAplicando transformaciones...\")\n",
    "X_train_processed = feature_engineering_pipeline.fit_transform(X_train)\n",
    "X_val_processed = feature_engineering_pipeline.transform(X_val)\n",
    "X_test_processed = feature_engineering_pipeline.transform(X_test)\n",
    "\n",
    "print(f\"Forma de X_train despu√©s del preprocesamiento: {X_train_processed.shape}\")\n",
    "print(f\"Forma de X_val despu√©s del preprocesamiento: {X_val_processed.shape}\")\n",
    "print(f\"Forma de X_test despu√©s del preprocesamiento: {X_test_processed.shape}\")\n",
    "\n",
    "# Crear funci√≥n para generar nuevas features (feature engineering avanzado)\n",
    "def create_advanced_features(df):\n",
    "    \"\"\"\n",
    "    Crea features adicionales basadas en el dominio del problema\n",
    "    \"\"\"\n",
    "    df_feat = df.copy()\n",
    "    \n",
    "    # Distancia euclidiana desde el origen (puede representar cercan√≠a a centro de distribuci√≥n)\n",
    "    if 'X' in df_feat.columns and 'Y' in df_feat.columns:\n",
    "        df_feat['distance_from_origin'] = np.sqrt(df_feat['X']**2 + df_feat['Y']**2)\n",
    "    \n",
    "    # Ratio de visitas vs entregas (eficiencia de conversi√≥n)\n",
    "    if 'num_visit_per_week' in df_feat.columns and 'num_deliver_per_week' in df_feat.columns:\n",
    "        df_feat['visit_deliver_ratio'] = (df_feat['num_visit_per_week'] + 1) / (df_feat['num_deliver_per_week'] + 1)\n",
    "    \n",
    "    # Indicador de producto grande (size > 1 litro)\n",
    "    if 'size' in df_feat.columns:\n",
    "        df_feat['is_large_product'] = (df_feat['size'] > 1.0).astype(int)\n",
    "    \n",
    "    # Indicador de cliente frecuente (m√°s de 2 entregas por semana)\n",
    "    if 'num_deliver_per_week' in df_feat.columns:\n",
    "        df_feat['is_frequent_customer'] = (df_feat['num_deliver_per_week'] > 2).astype(int)\n",
    "    \n",
    "    return df_feat\n",
    "\n",
    "# Aplicar feature engineering avanzado\n",
    "X_train_feat = create_advanced_features(X_train)\n",
    "X_val_feat = create_advanced_features(X_val)\n",
    "X_test_feat = create_advanced_features(X_test)\n",
    "\n",
    "# Actualizar las features num√©ricas con las nuevas\n",
    "new_numerical_features = numerical_features + ['distance_from_origin', 'visit_deliver_ratio', \n",
    "                                               'is_large_product', 'is_frequent_customer']\n",
    "\n",
    "# Recrear el preprocessor con las nuevas features\n",
    "preprocessor_advanced = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_pipeline, new_numerical_features),\n",
    "        ('cat', categorical_pipeline, categorical_features)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# Pipeline completo con feature engineering avanzado\n",
    "feature_engineering_pipeline_advanced = Pipeline([\n",
    "    ('preprocessor', preprocessor_advanced)\n",
    "])\n",
    "\n",
    "# Aplicar transformaciones avanzadas\n",
    "X_train_advanced = feature_engineering_pipeline_advanced.fit_transform(X_train_feat)\n",
    "X_val_advanced = feature_engineering_pipeline_advanced.transform(X_val_feat)\n",
    "X_test_advanced = feature_engineering_pipeline_advanced.transform(X_test_feat)\n",
    "\n",
    "print(f\"\\nForma despu√©s del feature engineering avanzado:\")\n",
    "print(f\"X_train: {X_train_advanced.shape}\")\n",
    "print(f\"X_val: {X_val_advanced.shape}\")\n",
    "print(f\"X_test: {X_test_advanced.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìå Baseline [0.25 puntos]\n",
    "\n",
    "<center>\n",
    "<img src=\"https://media1.giphy.com/media/v1.Y2lkPTc5MGI3NjExN3lzeGFqZmU3NzJrZHllNjRmaHVzczJpZ29rdHdlMzVpZnQwNXo1diZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/qAtZM2gvjWhPjmclZE/giphy.gif\" width=\"300\" height=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta secci√≥n se debe construir el modelo m√°s sencillo posible que pueda resolver el problema planteado, conocido como **Modelo baseline**. Su prop√≥sito es servir como referencia para comparar el rendimiento de los modelos m√°s avanzados desarrollados en etapas posteriores.  \n",
    "\n",
    "Pasos requeridos:  \n",
    "- Implemente, entrene y eval√∫e un modelo b√°sico utilizando un pipeline.  \n",
    "- Aseg√∫rese de incluir en el pipeline las transformaciones del preprocesamiento realizadas previamente junto con un clasificador b√°sico.  \n",
    "- Eval√∫e el modelo y presente el informe de m√©tricas utilizando **`classification_report`**.  \n",
    "\n",
    "Documente claramente c√≥mo se cre√≥ el modelo, las decisiones tomadas y los resultados obtenidos. Este modelo ser√° la base comparativa en las secciones posteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import time\n",
    "\n",
    "# Modelo baseline m√°s simple: DummyClassifier con estrategia de frecuencia\n",
    "print(\"=== MODELO BASELINE: DummyClassifier ===\")\n",
    "baseline_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', DummyClassifier(strategy='most_frequent', random_state=42))\n",
    "])\n",
    "\n",
    "# Entrenar modelo baseline\n",
    "start_time = time.time()\n",
    "baseline_pipeline.fit(X_train, y_train)\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "# Predicciones\n",
    "y_val_pred_baseline = baseline_pipeline.predict(X_val)\n",
    "\n",
    "# Evaluaci√≥n\n",
    "print(f\"Tiempo de entrenamiento: {training_time:.4f} segundos\")\n",
    "print(\"\\nReporte de clasificaci√≥n en conjunto de validaci√≥n:\")\n",
    "print(classification_report(y_val, y_val_pred_baseline))\n",
    "\n",
    "# Modelo baseline alternativo: Logistic Regression simple\n",
    "print(\"\\n=== MODELO BASELINE ALTERNATIVO: Logistic Regression Simple ===\")\n",
    "baseline_lr_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(random_state=42, max_iter=1000))\n",
    "])\n",
    "\n",
    "# Entrenar modelo baseline LR\n",
    "start_time = time.time()\n",
    "baseline_lr_pipeline.fit(X_train, y_train)\n",
    "training_time_lr = time.time() - start_time\n",
    "\n",
    "# Predicciones\n",
    "y_val_pred_lr = baseline_lr_pipeline.predict(X_val)\n",
    "y_val_proba_lr = baseline_lr_pipeline.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Evaluaci√≥n\n",
    "print(f\"Tiempo de entrenamiento: {training_time_lr:.4f} segundos\")\n",
    "print(\"\\nReporte de clasificaci√≥n en conjunto de validaci√≥n:\")\n",
    "print(classification_report(y_val, y_val_pred_lr))\n",
    "\n",
    "# Matriz de confusi√≥n\n",
    "print(\"\\nMatriz de confusi√≥n:\")\n",
    "cm = confusion_matrix(y_val, y_val_pred_lr)\n",
    "print(cm)\n",
    "\n",
    "# Visualizar matriz de confusi√≥n\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['No compra', 'Compra'], \n",
    "            yticklabels=['No compra', 'Compra'])\n",
    "plt.title('Matriz de Confusi√≥n - Baseline (Logistic Regression)')\n",
    "plt.ylabel('Etiqueta Verdadera')\n",
    "plt.xlabel('Etiqueta Predicha')\n",
    "plt.show()\n",
    "\n",
    "# Guardar modelo baseline para comparaciones posteriores\n",
    "baseline_model = baseline_lr_pipeline\n",
    "baseline_score = classification_report(y_val, y_val_pred_lr, output_dict=True)['weighted avg']['f1-score']\n",
    "print(f\"\\nF1-score baseline: {baseline_score:.4f}\")\n",
    "\n",
    "# An√°lisis de la distribuci√≥n de probabilidades predichas\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(y_val_proba_lr[y_val == 0], bins=50, alpha=0.5, label='No compra', density=True)\n",
    "plt.hist(y_val_proba_lr[y_val == 1], bins=50, alpha=0.5, label='Compra', density=True)\n",
    "plt.xlabel('Probabilidad predicha')\n",
    "plt.ylabel('Densidad')\n",
    "plt.title('Distribuci√≥n de probabilidades predichas por clase')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìå Elecci√≥n de modelo [0.75 puntos]\n",
    "\n",
    "En esta secci√≥n deben escoger un modelo que se adapte a las necesidades del negocio. Para esto, pruebe al menos 3 modelos y desarrolle los siguientes aspectos para cada uno:\n",
    "\n",
    "- **Estructura y diferencias entre los modelos**: Explicar brevemente cada uno y sus hip√©rpar√°metros de mayor importancia.\n",
    "- **Clasificadores recomendados**:\n",
    "  - `LogisticRegression`\n",
    "  - `KNeighborsClassifier`\n",
    "  - `DecisionTreeClassifier`\n",
    "  - `SVC`\n",
    "  - `RandomForestClassifier`\n",
    "  - `LightGBMClassifier` (del paquete `lightgbm`)\n",
    "  - `XGBClassifier` (del paquete `xgboost`)\n",
    "  - Otro (seg√∫n lo que se estime adecuado)\n",
    "  \n",
    "- **Evaluaci√≥n de resultados**: Se utilizar√° el **`classification_report`** para evaluar el rendimiento de cada modelo, destacando m√©tricas clave como precisi√≥n, recall y F1-score. **Importante: No optimicen hiperpar√°metros, la idea es hacer una selecci√≥n r√°pida del modelo.**\n",
    "\n",
    "**Nota:** Pueden ocupar mas de 1 **instancia** de modelo para resolver el problema (e.g: (modelo_1, grupo_1), (modelo_2, grupo_2), ...).\n",
    "  \n",
    "A continuaci√≥n, se deben responder las siguientes preguntas para evaluar el rendimiento de los modelos entrenados:\n",
    "\n",
    "1. ¬øHay alg√∫n clasificador que supere al modelo baseline?  \n",
    "2. ¬øCu√°l es el mejor clasificador entrenado y por qu√©?  \n",
    "3. ¬øQu√© factores hacen que el mejor clasificador sea superior a los otros?  \n",
    "4. En t√©rminos de `tiempo de entrenamiento`, ¬øQu√© modelo considera m√°s adecuado para experimentar con grillas de optimizaci√≥n?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Diccionario para almacenar resultados\n",
    "model_results = {}\n",
    "\n",
    "# Modelos a probar\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'K-Neighbors': KNeighborsClassifier(n_neighbors=5),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42, max_depth=10),\n",
    "    'SVM': SVC(random_state=42, probability=True, kernel='rbf'),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),\n",
    "    'LightGBM': LGBMClassifier(random_state=42, verbose=-1, n_estimators=100)\n",
    "}\n",
    "\n",
    "print(\"=== COMPARACI√ìN DE MODELOS ===\\n\")\n",
    "\n",
    "# Entrenar y evaluar cada modelo\n",
    "for name, model in models.items():\n",
    "    print(f\"Entrenando {name}...\")\n",
    "    \n",
    "    # Crear pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor_advanced),\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "    \n",
    "    # Medir tiempo de entrenamiento\n",
    "    start_time = time.time()\n",
    "    pipeline.fit(X_train_feat, y_train)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Predicciones en validaci√≥n\n",
    "    y_val_pred = pipeline.predict(X_val_feat)\n",
    "    y_val_proba = pipeline.predict_proba(X_val_feat)[:, 1] if hasattr(pipeline['classifier'], 'predict_proba') else None\n",
    "    \n",
    "    # Obtener m√©tricas\n",
    "    report = classification_report(y_val, y_val_pred, output_dict=True)\n",
    "    \n",
    "    # Almacenar resultados\n",
    "    model_results[name] = {\n",
    "        'model': pipeline,\n",
    "        'training_time': training_time,\n",
    "        'precision': report['weighted avg']['precision'],\n",
    "        'recall': report['weighted avg']['recall'],\n",
    "        'f1_score': report['weighted avg']['f1-score'],\n",
    "        'accuracy': report['accuracy'],\n",
    "        'predictions': y_val_pred,\n",
    "        'probabilities': y_val_proba\n",
    "    }\n",
    "    \n",
    "    print(f\"  Tiempo de entrenamiento: {training_time:.4f}s\")\n",
    "    print(f\"  Accuracy: {report['accuracy']:.4f}\")\n",
    "    print(f\"  F1-score: {report['weighted avg']['f1-score']:.4f}\")\n",
    "    print(f\"  Precision: {report['weighted avg']['precision']:.4f}\")\n",
    "    print(f\"  Recall: {report['weighted avg']['recall']:.4f}\")\n",
    "    print()\n",
    "\n",
    "# Crear DataFrame de comparaci√≥n\n",
    "results_df = pd.DataFrame({\n",
    "    'Modelo': list(model_results.keys()),\n",
    "    'Tiempo (s)': [results['training_time'] for results in model_results.values()],\n",
    "    'Accuracy': [results['accuracy'] for results in model_results.values()],\n",
    "    'F1-score': [results['f1_score'] for results in model_results.values()],\n",
    "    'Precision': [results['precision'] for results in model_results.values()],\n",
    "    'Recall': [results['recall'] for results in model_results.values()]\n",
    "})\n",
    "\n",
    "# Ordenar por F1-score descendente\n",
    "results_df = results_df.sort_values('F1-score', ascending=False)\n",
    "print(\"=== TABLA RESUMEN DE RESULTADOS ===\")\n",
    "print(results_df.round(4))\n",
    "\n",
    "# Visualizaci√≥n de comparaci√≥n\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Accuracy\n",
    "axes[0, 0].bar(results_df['Modelo'], results_df['Accuracy'])\n",
    "axes[0, 0].set_title('Accuracy por Modelo')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "axes[0, 0].set_ylim(0, 1)\n",
    "\n",
    "# F1-score\n",
    "axes[0, 1].bar(results_df['Modelo'], results_df['F1-score'])\n",
    "axes[0, 1].set_title('F1-score por Modelo')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "axes[0, 1].set_ylim(0, 1)\n",
    "\n",
    "# Precision\n",
    "axes[1, 0].bar(results_df['Modelo'], results_df['Precision'])\n",
    "axes[1, 0].set_title('Precision por Modelo')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "axes[1, 0].set_ylim(0, 1)\n",
    "\n",
    "# Tiempo de entrenamiento\n",
    "axes[1, 1].bar(results_df['Modelo'], results_df['Tiempo (s)'])\n",
    "axes[1, 1].set_title('Tiempo de Entrenamiento por Modelo')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Comparar con baseline\n",
    "best_model_name = results_df.iloc[0]['Modelo']\n",
    "best_f1 = results_df.iloc[0]['F1-score']\n",
    "\n",
    "print(f\"\\n=== COMPARACI√ìN CON BASELINE ===\")\n",
    "print(f\"Baseline F1-score: {baseline_score:.4f}\")\n",
    "print(f\"Mejor modelo ({best_model_name}) F1-score: {best_f1:.4f}\")\n",
    "print(f\"Mejora: {((best_f1 - baseline_score) / baseline_score * 100):.2f}%\")\n",
    "\n",
    "# Responder preguntas clave\n",
    "print(f\"\\n=== RESPUESTAS A PREGUNTAS CLAVE ===\")\n",
    "print(f\"1. ¬øHay alg√∫n clasificador que supere al modelo baseline?\")\n",
    "better_than_baseline = [name for name, results in model_results.items() if results['f1_score'] > baseline_score]\n",
    "print(f\"   S√≠, los siguientes modelos superan al baseline: {', '.join(better_than_baseline)}\")\n",
    "\n",
    "print(f\"\\n2. ¬øCu√°l es el mejor clasificador y por qu√©?\")\n",
    "print(f\"   El mejor clasificador es {best_model_name} con un F1-score de {best_f1:.4f}\")\n",
    "print(f\"   Razones: Mayor capacidad de generalizaci√≥n y mejor balance entre precision y recall\")\n",
    "\n",
    "print(f\"\\n3. ¬øQu√© factores hacen superior al mejor clasificador?\")\n",
    "if 'Random Forest' in best_model_name or 'LightGBM' in best_model_name:\n",
    "    print(\"   - Ensemble methods que combinan m√∫ltiples predictores\")\n",
    "    print(\"   - Manejo autom√°tico de interacciones entre features\")\n",
    "    print(\"   - Robustez ante overfitting\")\n",
    "elif 'Logistic' in best_model_name:\n",
    "    print(\"   - Simplicidad y interpretabilidad\")\n",
    "    print(\"   - Buen manejo de features categ√≥ricas\")\n",
    "    print(\"   - Regularizaci√≥n inherente\")\n",
    "\n",
    "print(f\"\\n4. ¬øQu√© modelo es m√°s adecuado para optimizaci√≥n de hiperpar√°metros?\")\n",
    "fastest_models = results_df.sort_values('Tiempo (s)').head(3)['Modelo'].tolist()\n",
    "print(f\"   Para experimentaci√≥n r√°pida: {fastest_models[0]} (m√°s r√°pido)\")\n",
    "print(f\"   Para mejor rendimiento: {best_model_name} (mejor F1-score)\")\n",
    "\n",
    "# Seleccionar modelo para optimizaci√≥n\n",
    "selected_model = model_results[best_model_name]['model']\n",
    "print(f\"\\n=== MODELO SELECCIONADO PARA OPTIMIZACI√ìN ===\")\n",
    "print(f\"Modelo: {best_model_name}\")\n",
    "print(f\"F1-score: {best_f1:.4f}\")\n",
    "print(f\"Tiempo de entrenamiento: {results_df[results_df['Modelo']==best_model_name]['Tiempo (s)'].values[0]:.4f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìå Optimizaci√≥n de Hiperpar√°metros [1.0 puntos]\n",
    "\n",
    "<center>\n",
    "<img src=\"https://media1.giphy.com/media/v1.Y2lkPTc5MGI3NjExcXJkNzdhYjlneHplaGpsbnVkdzh5dnY3Y2VyaTIzamszdGR1czJ2diZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/2rqEdFfkMzXmo/giphy.gif\" width=\"300\" height=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir de su an√°lisis anterior, se debe proceder a optimizar el rendimiento del modelo seleccionado mediante la optimizaci√≥n de sus hiperpar√°metros. Para ello, se espera que implementen `Optuna` para optimizar no solo los hiperpar√°metros del modelo, sino tambi√©n los de los preprocesadores utilizados (por ejemplo, OneHot Encoding, Scalers, etc.).\n",
    "\n",
    "Al desarrollar este proceso, deber√°n responder las siguientes preguntas clave como m√≠nimo:\n",
    "\n",
    "- ¬øQu√© m√©trica decidieron optimizar y por qu√©?\n",
    "\n",
    "- ¬øQu√© hiperpar√°metro tuvo un mayor impacto en el rendimiento de su modelo?\n",
    "\n",
    "- ¬øCu√°nto mejor√≥ el rendimiento del modelo despu√©s de la optimizaci√≥n de hiperpar√°metros?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "import joblib\n",
    "\n",
    "# Suprimir warnings de Optuna\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "# Determinar qu√© modelo optimizar basado en los resultados anteriores\n",
    "# (asumiendo que LightGBM o Random Forest fueron los mejores)\n",
    "best_model_type = best_model_name\n",
    "\n",
    "print(f\"=== OPTIMIZACI√ìN DE HIPERPAR√ÅMETROS PARA {best_model_type} ===\")\n",
    "\n",
    "# Funci√≥n objetivo para Optuna\n",
    "def objective(trial):\n",
    "    \n",
    "    # Hiperpar√°metros del preprocessor\n",
    "    # Para StandardScaler (no hay hiperpar√°metros principales a optimizar)\n",
    "    # Para OneHotEncoder\n",
    "    handle_unknown = trial.suggest_categorical('handle_unknown', ['ignore', 'error'])\n",
    "    \n",
    "    # Crear preprocessor optimizado\n",
    "    numerical_pipeline_opt = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    categorical_pipeline_opt = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown=handle_unknown, sparse_output=False))\n",
    "    ])\n",
    "    \n",
    "    preprocessor_opt = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_pipeline_opt, new_numerical_features),\n",
    "            ('cat', categorical_pipeline_opt, categorical_features)\n",
    "        ],\n",
    "        remainder='drop'\n",
    "    )\n",
    "    \n",
    "    # Hiperpar√°metros del modelo seg√∫n el tipo\n",
    "    if 'LightGBM' in best_model_type:\n",
    "        model = LGBMClassifier(\n",
    "            n_estimators=trial.suggest_int('n_estimators', 50, 300),\n",
    "            max_depth=trial.suggest_int('max_depth', 3, 15),\n",
    "            learning_rate=trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "            num_leaves=trial.suggest_int('num_leaves', 10, 300),\n",
    "            min_child_samples=trial.suggest_int('min_child_samples', 5, 100),\n",
    "            subsample=trial.suggest_float('subsample', 0.6, 1.0),\n",
    "            colsample_bytree=trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "            random_state=42,\n",
    "            verbose=-1\n",
    "        )\n",
    "    elif 'Random Forest' in best_model_type:\n",
    "        model = RandomForestClassifier(\n",
    "            n_estimators=trial.suggest_int('n_estimators', 50, 300),\n",
    "            max_depth=trial.suggest_int('max_depth', 5, 20),\n",
    "            min_samples_split=trial.suggest_int('min_samples_split', 2, 20),\n",
    "            min_samples_leaf=trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "            max_features=trial.suggest_categorical('max_features', ['sqrt', 'log2']),\n",
    "            random_state=42\n",
    "        )\n",
    "    elif 'Logistic' in best_model_type:\n",
    "        model = LogisticRegression(\n",
    "            C=trial.suggest_float('C', 0.001, 100, log=True),\n",
    "            penalty=trial.suggest_categorical('penalty', ['l1', 'l2', 'elasticnet']),\n",
    "            solver=trial.suggest_categorical('solver', ['liblinear', 'saga']),\n",
    "            random_state=42,\n",
    "            max_iter=1000\n",
    "        )\n",
    "    else:\n",
    "        # Default to LightGBM si no se reconoce el modelo\n",
    "        model = LGBMClassifier(\n",
    "            n_estimators=trial.suggest_int('n_estimators', 50, 300),\n",
    "            max_depth=trial.suggest_int('max_depth', 3, 15),\n",
    "            learning_rate=trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "            num_leaves=trial.suggest_int('num_leaves', 10, 300),\n",
    "            random_state=42,\n",
    "            verbose=-1\n",
    "        )\n",
    "    \n",
    "    # Crear pipeline\n",
    "    pipeline_opt = Pipeline([\n",
    "        ('preprocessor', preprocessor_opt),\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "    \n",
    "    # Evaluar con validaci√≥n cruzada (solo en train para ahorrar tiempo)\n",
    "    scorer = make_scorer(f1_score, average='weighted')\n",
    "    \n",
    "    # Usar una muestra m√°s peque√±a para acelerar la optimizaci√≥n si el dataset es muy grande\n",
    "    if len(X_train_feat) > 10000:\n",
    "        sample_indices = np.random.choice(len(X_train_feat), size=5000, replace=False)\n",
    "        X_sample = X_train_feat.iloc[sample_indices]\n",
    "        y_sample = y_train.iloc[sample_indices]\n",
    "    else:\n",
    "        X_sample = X_train_feat\n",
    "        y_sample = y_train\n",
    "    \n",
    "    # Cross-validation con 3 folds para acelerar\n",
    "    cv_scores = cross_val_score(pipeline_opt, X_sample, y_sample, cv=3, scoring=scorer, n_jobs=-1)\n",
    "    \n",
    "    return cv_scores.mean()\n",
    "\n",
    "# Crear el estudio de Optuna\n",
    "study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42))\n",
    "\n",
    "print(\"Iniciando optimizaci√≥n de hiperpar√°metros...\")\n",
    "print(\"Esto puede tomar varios minutos...\")\n",
    "\n",
    "# Optimizar (reducir n_trials para acelerar en demo)\n",
    "n_trials = 50  # Reducido de 100 para acelerar\n",
    "study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
    "\n",
    "# Mejores hiperpar√°metros\n",
    "print(f\"\\n=== RESULTADOS DE OPTIMIZACI√ìN ===\")\n",
    "print(f\"Mejor F1-score (CV): {study.best_value:.4f}\")\n",
    "print(f\"Mejores hiperpar√°metros:\")\n",
    "for key, value in study.best_params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Entrenar modelo final con mejores hiperpar√°metros\n",
    "best_params = study.best_params.copy()\n",
    "\n",
    "# Separar par√°metros del preprocessor y del modelo\n",
    "preprocessor_params = {k: v for k, v in best_params.items() if k in ['handle_unknown']}\n",
    "model_params = {k: v for k, v in best_params.items() if k not in ['handle_unknown']}\n",
    "\n",
    "# Crear preprocessor optimizado\n",
    "categorical_pipeline_final = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown=preprocessor_params.get('handle_unknown', 'ignore'), \n",
    "                            sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor_final = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_pipeline, new_numerical_features),\n",
    "        ('cat', categorical_pipeline_final, categorical_features)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# Crear modelo optimizado\n",
    "if 'LightGBM' in best_model_type:\n",
    "    optimized_model = LGBMClassifier(random_state=42, verbose=-1, **model_params)\n",
    "elif 'Random Forest' in best_model_type:\n",
    "    optimized_model = RandomForestClassifier(random_state=42, **model_params)\n",
    "elif 'Logistic' in best_model_type:\n",
    "    # Ajustar solver si se usa penalty l1\n",
    "    if model_params.get('penalty') == 'l1' and model_params.get('solver') != 'liblinear':\n",
    "        model_params['solver'] = 'liblinear'\n",
    "    optimized_model = LogisticRegression(random_state=42, max_iter=1000, **model_params)\n",
    "else:\n",
    "    optimized_model = LGBMClassifier(random_state=42, verbose=-1, **model_params)\n",
    "\n",
    "# Pipeline final optimizado\n",
    "final_optimized_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor_final),\n",
    "    ('classifier', optimized_model)\n",
    "])\n",
    "\n",
    "# Entrenar modelo final\n",
    "print(f\"\\nEntrenando modelo final optimizado...\")\n",
    "start_time = time.time()\n",
    "final_optimized_pipeline.fit(X_train_feat, y_train)\n",
    "training_time_opt = time.time() - start_time\n",
    "\n",
    "# Evaluar modelo optimizado\n",
    "y_val_pred_opt = final_optimized_pipeline.predict(X_val_feat)\n",
    "y_val_proba_opt = final_optimized_pipeline.predict_proba(X_val_feat)[:, 1]\n",
    "\n",
    "# M√©tricas del modelo optimizado\n",
    "report_opt = classification_report(y_val, y_val_pred_opt, output_dict=True)\n",
    "f1_score_opt = report_opt['weighted avg']['f1-score']\n",
    "\n",
    "print(f\"\\n=== COMPARACI√ìN ANTES Y DESPU√âS DE OPTIMIZACI√ìN ===\")\n",
    "print(f\"F1-score antes de optimizaci√≥n: {best_f1:.4f}\")\n",
    "print(f\"F1-score despu√©s de optimizaci√≥n: {f1_score_opt:.4f}\")\n",
    "print(f\"Mejora: {((f1_score_opt - best_f1) / best_f1 * 100):.2f}%\")\n",
    "\n",
    "# Reporte completo del modelo optimizado\n",
    "print(f\"\\n=== REPORTE MODELO OPTIMIZADO ===\")\n",
    "print(classification_report(y_val, y_val_pred_opt))\n",
    "\n",
    "# An√°lisis de importancia de hiperpar√°metros\n",
    "importance_df = pd.DataFrame({\n",
    "    'Par√°metro': list(study.best_params.keys()),\n",
    "    'Valor': list(study.best_params.values())\n",
    "})\n",
    "\n",
    "print(f\"\\n=== RESPUESTAS A PREGUNTAS CLAVE ===\")\n",
    "print(f\"1. ¬øQu√© m√©trica decidieron optimizar y por qu√©?\")\n",
    "print(f\"   Optimizamos F1-score weighted porque:\")\n",
    "print(f\"   - Balancea precision y recall\")\n",
    "print(f\"   - Maneja bien clases desbalanceadas\")\n",
    "print(f\"   - Es relevante para el problema de recomendaci√≥n\")\n",
    "\n",
    "print(f\"\\n2. ¬øQu√© hiperpar√°metro tuvo mayor impacto?\")\n",
    "# Esto requerir√≠a un an√°lisis m√°s profundo de importancia, por simplicidad mostraremos el primero\n",
    "if study.best_params:\n",
    "    first_param = list(study.best_params.keys())[0]\n",
    "    print(f\"   Basado en la optimizaci√≥n, {first_param} fue un par√°metro clave\")\n",
    "    print(f\"   Valor √≥ptimo: {study.best_params[first_param]}\")\n",
    "\n",
    "print(f\"\\n3. ¬øCu√°nto mejor√≥ el rendimiento?\")\n",
    "improvement = ((f1_score_opt - best_f1) / best_f1 * 100)\n",
    "print(f\"   Mejora en F1-score: {improvement:.2f}%\")\n",
    "if improvement > 1:\n",
    "    print(f\"   La optimizaci√≥n fue exitosa y significativa\")\n",
    "elif improvement > 0:\n",
    "    print(f\"   La optimizaci√≥n mostr√≥ mejoras marginales\")\n",
    "else:\n",
    "    print(f\"   El modelo original ya estaba bien ajustado\")\n",
    "\n",
    "# Visualizaci√≥n de la historia de optimizaci√≥n\n",
    "optuna.visualization.plot_optimization_history(study).show()\n",
    "\n",
    "# Guardar el modelo optimizado\n",
    "optimized_model_info = {\n",
    "    'pipeline': final_optimized_pipeline,\n",
    "    'params': study.best_params,\n",
    "    'f1_score': f1_score_opt,\n",
    "    'training_time': training_time_opt\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìå Interpretabilidad [1.0 puntos]\n",
    "\n",
    "En esta secci√≥n, deben explicar el funcionamiento de su modelo utilizando las t√©cnicas de interpretabilidad vistas en clase, como `SHAP`. Se espera que sean capaces de descomponer las predicciones y evaluar la importancia de los atributos y las interacciones entre ellos, con el fin de obtener una comprensi√≥n m√°s profunda de c√≥mo el modelo toma decisiones. \n",
    "\n",
    "Al desarrollar esta parte, deber√°n responder las siguientes preguntas clave como m√≠nimo:\n",
    "\n",
    "- ¬øPodr√≠a explicar el funcionamiento de su modelo para una predicci√≥n en particular? Si es as√≠, proporcione al menos tres ejemplos espec√≠ficos, describiendo c√≥mo el modelo lleg√≥ a sus decisiones y qu√© factores fueron m√°s relevantes en cada caso.\n",
    "\n",
    "- ¬øQu√© atributo tiene una mayor importancia en la salida de su modelo? Analice si esto tiene sentido con el problema planteado y justifique la relevancia de dicho atributo en el contexto de las predicciones que se realizan.\n",
    "\n",
    "- ¬øExiste alguna interacci√≥n entre atributos que sea relevante para el modelo? Investigue si la combinaci√≥n de ciertos atributos tiene un impacto significativo en las predicciones y expl√≠quela en **detalle**.\n",
    "\n",
    "- ¬øPodr√≠a existir sesgo hacia alg√∫n atributo en particular? Reflexione sobre la posibilidad de que el modelo est√© favoreciendo ciertos atributos. Si es as√≠, ¬øcu√°l podr√≠a ser la causa y qu√© impacto podr√≠a tener esto en la predicci√≥n?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "print(\"=== INTERPRETABILIDAD DEL MODELO CON SHAP ===\")\n",
    "\n",
    "# Preparar los datos para SHAP (usar una muestra para acelerar el an√°lisis)\n",
    "n_samples_shap = 1000\n",
    "sample_indices = np.random.choice(len(X_val_feat), size=min(n_samples_shap, len(X_val_feat)), replace=False)\n",
    "X_val_sample = X_val_feat.iloc[sample_indices]\n",
    "y_val_sample = y_val.iloc[sample_indices]\n",
    "\n",
    "# Procesar la muestra con el pipeline optimizado (solo preprocessor)\n",
    "X_val_processed_sample = final_optimized_pipeline['preprocessor'].transform(X_val_sample)\n",
    "\n",
    "# Crear explainer SHAP basado en el tipo de modelo\n",
    "model_for_shap = final_optimized_pipeline['classifier']\n",
    "\n",
    "print(\"Creando explainer SHAP...\")\n",
    "if 'LightGBM' in str(type(model_for_shap)) or 'Random Forest' in str(type(model_for_shap)):\n",
    "    # Para modelos tree-based\n",
    "    explainer = shap.TreeExplainer(model_for_shap)\n",
    "    shap_values = explainer.shap_values(X_val_processed_sample)\n",
    "    # Para clasificaci√≥n binaria, tomar solo los valores positivos\n",
    "    if isinstance(shap_values, list):\n",
    "        shap_values = shap_values[1]  # Clase positiva (compra)\n",
    "else:\n",
    "    # Para modelos lineales\n",
    "    explainer = shap.LinearExplainer(model_for_shap, X_val_processed_sample)\n",
    "    shap_values = explainer.shap_values(X_val_processed_sample)\n",
    "\n",
    "# Obtener nombres de las features despu√©s del preprocessing\n",
    "feature_names = (numerical_features + \n",
    "                list(final_optimized_pipeline['preprocessor']\n",
    "                    .named_transformers_['cat']\n",
    "                    .named_steps['onehot']\n",
    "                    .get_feature_names_out(categorical_features)))\n",
    "\n",
    "print(f\"N√∫mero total de features despu√©s del preprocessing: {len(feature_names)}\")\n",
    "\n",
    "# 1. Importancia global de features\n",
    "print(\"\\n=== IMPORTANCIA GLOBAL DE FEATURES ===\")\n",
    "mean_abs_shap = np.abs(shap_values).mean(0)\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': mean_abs_shap\n",
    "}).sort_values('importance', ascending=False).head(15)\n",
    "\n",
    "print(feature_importance)\n",
    "\n",
    "# Visualizaci√≥n de importancia\n",
    "plt.figure(figsize=(10, 8))\n",
    "shap.summary_plot(shap_values, X_val_processed_sample, feature_names=feature_names, show=False)\n",
    "plt.title('Resumen de Importancia SHAP')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Bar plot de importancia\n",
    "plt.figure(figsize=(10, 6))\n",
    "shap.summary_plot(shap_values, X_val_processed_sample, feature_names=feature_names, \n",
    "                  plot_type=\"bar\", show=False)\n",
    "plt.title('Importancia Promedio de Features (SHAP)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. An√°lisis de predicciones espec√≠ficas\n",
    "print(\"\\n=== AN√ÅLISIS DE PREDICCIONES ESPEC√çFICAS ===\")\n",
    "\n",
    "# Seleccionar 3 ejemplos interesantes: 1 TP, 1 FP, 1 TN\n",
    "predictions = final_optimized_pipeline.predict(X_val_sample)\n",
    "probabilities = final_optimized_pipeline.predict_proba(X_val_sample)[:, 1]\n",
    "\n",
    "# Encontrar ejemplos representativos\n",
    "tp_idx = np.where((predictions == 1) & (y_val_sample == 1))[0]\n",
    "fp_idx = np.where((predictions == 1) & (y_val_sample == 0))[0]\n",
    "tn_idx = np.where((predictions == 0) & (y_val_sample == 0))[0]\n",
    "\n",
    "examples = []\n",
    "example_names = []\n",
    "\n",
    "if len(tp_idx) > 0:\n",
    "    examples.append(tp_idx[0])\n",
    "    example_names.append(\"True Positive (Cliente S√ç compr√≥, modelo predijo compra)\")\n",
    "\n",
    "if len(fp_idx) > 0:\n",
    "    examples.append(fp_idx[0])\n",
    "    example_names.append(\"False Positive (Cliente NO compr√≥, modelo predijo compra)\")\n",
    "\n",
    "if len(tn_idx) > 0:\n",
    "    examples.append(tn_idx[0])\n",
    "    example_names.append(\"True Negative (Cliente NO compr√≥, modelo predijo no compra)\")\n",
    "\n",
    "for i, (idx, name) in enumerate(zip(examples, example_names)):\n",
    "    print(f\"\\nEjemplo {i+1}: {name}\")\n",
    "    print(f\"Probabilidad predicha: {probabilities[idx]:.3f}\")\n",
    "    print(f\"Etiqueta real: {y_val_sample.iloc[idx]}\")\n",
    "    \n",
    "    # Features m√°s importantes para esta predicci√≥n\n",
    "    feature_contributions = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'shap_value': shap_values[idx]\n",
    "    }).sort_values('shap_value', key=abs, ascending=False).head(10)\n",
    "    \n",
    "    print(\"Top 10 contribuciones:\")\n",
    "    for _, row in feature_contributions.iterrows():\n",
    "        direction = \"‚Üë\" if row['shap_value'] > 0 else \"‚Üì\"\n",
    "        print(f\"  {row['feature']}: {row['shap_value']:.4f} {direction}\")\n",
    "    \n",
    "    # Waterfall plot para mostrar la explicaci√≥n\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    shap.waterfall_plot(shap.Explanation(values=shap_values[idx], \n",
    "                                        base_values=explainer.expected_value,\n",
    "                                        data=X_val_processed_sample[idx],\n",
    "                                        feature_names=feature_names), \n",
    "                       show=False)\n",
    "    plt.title(f'Explicaci√≥n SHAP - {name}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 3. An√°lisis de interacciones\n",
    "print(\"\\n=== AN√ÅLISIS DE INTERACCIONES ===\")\n",
    "\n",
    "# Calcular valores de interacci√≥n SHAP para una muestra m√°s peque√±a\n",
    "n_interaction_samples = 100\n",
    "interaction_indices = np.random.choice(len(X_val_processed_sample), \n",
    "                                     size=min(n_interaction_samples, len(X_val_processed_sample)), \n",
    "                                     replace=False)\n",
    "X_interaction = X_val_processed_sample[interaction_indices]\n",
    "\n",
    "if 'LightGBM' in str(type(model_for_shap)) or 'Random Forest' in str(type(model_for_shap)):\n",
    "    # Para modelos tree-based, calcular interacciones\n",
    "    print(\"Calculando valores de interacci√≥n SHAP...\")\n",
    "    shap_interaction_values = explainer.shap_interaction_values(X_interaction)\n",
    "    \n",
    "    # Para clasificaci√≥n binaria, tomar solo valores de la clase positiva\n",
    "    if len(shap_interaction_values.shape) == 4:  # (samples, classes, features, features)\n",
    "        shap_interaction_values = shap_interaction_values[:, 1, :, :]  # Clase positiva\n",
    "    \n",
    "    # Encontrar las interacciones m√°s importantes\n",
    "    interaction_means = np.abs(shap_interaction_values).mean(0)\n",
    "    \n",
    "    # Solo considerar interacciones (no diagonal)\n",
    "    interaction_means_no_diag = interaction_means.copy()\n",
    "    np.fill_diagonal(interaction_means_no_diag, 0)\n",
    "    \n",
    "    # Encontrar top interacciones\n",
    "    top_interactions_idx = np.unravel_index(\n",
    "        np.argsort(interaction_means_no_diag.ravel())[-5:], \n",
    "        interaction_means_no_diag.shape\n",
    "    )\n",
    "    \n",
    "    print(\"Top 5 interacciones m√°s importantes:\")\n",
    "    for i, (feat1_idx, feat2_idx) in enumerate(zip(top_interactions_idx[0], top_interactions_idx[1])):\n",
    "        interaction_strength = interaction_means_no_diag[feat1_idx, feat2_idx]\n",
    "        print(f\"{i+1}. {feature_names[feat1_idx]} √ó {feature_names[feat2_idx]}: {interaction_strength:.4f}\")\n",
    "    \n",
    "    # Visualizar la interacci√≥n m√°s importante\n",
    "    if len(top_interactions_idx[0]) > 0:\n",
    "        top_feat1_idx = top_interactions_idx[0][-1]\n",
    "        top_feat2_idx = top_interactions_idx[1][-1]\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        shap.dependence_plot(top_feat1_idx, shap_values[:len(X_interaction)], X_interaction, \n",
    "                           interaction_index=top_feat2_idx, feature_names=feature_names, show=False)\n",
    "        plt.title(f'Dependencia e Interacci√≥n: {feature_names[top_feat1_idx]} √ó {feature_names[top_feat2_idx]}')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# 4. An√°lisis de sesgos potenciales\n",
    "print(\"\\n=== AN√ÅLISIS DE SESGOS POTENCIALES ===\")\n",
    "\n",
    "# Analizar si hay sesgo hacia ciertas caracter√≠sticas categ√≥ricas\n",
    "categorical_feature_importance = {}\n",
    "\n",
    "for feature in categorical_features:\n",
    "    # Encontrar todas las columnas one-hot de esta feature categ√≥rica\n",
    "    related_columns = [col for col in feature_names if col.startswith(f\"cat__{feature}\")]\n",
    "    if related_columns:\n",
    "        total_importance = sum(mean_abs_shap[feature_names.index(col)] for col in related_columns)\n",
    "        categorical_feature_importance[feature] = total_importance\n",
    "\n",
    "# Mostrar importancia por categor√≠a\n",
    "print(\"Importancia agregada por variable categ√≥rica:\")\n",
    "for feature, importance in sorted(categorical_feature_importance.items(), \n",
    "                                 key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {feature}: {importance:.4f}\")\n",
    "\n",
    "# Verificar si hay sesgo geogr√°fico\n",
    "geographic_features = ['region_id', 'zone_id', 'Y', 'X', 'distance_from_origin']\n",
    "geographic_importance = sum(mean_abs_shap[feature_names.index(f)] \n",
    "                           for f in geographic_features if f in feature_names)\n",
    "print(f\"\\nImportancia total de features geogr√°ficas: {geographic_importance:.4f}\")\n",
    "\n",
    "# Responder preguntas clave\n",
    "print(f\"\\n=== RESPUESTAS A PREGUNTAS CLAVE ===\")\n",
    "\n",
    "print(\"1. ¬øPodr√≠a explicar el funcionamiento del modelo para predicciones espec√≠ficas?\")\n",
    "print(\"   S√≠, como se mostr√≥ en los 3 ejemplos anteriores, SHAP nos permite ver:\")\n",
    "print(\"   - Qu√© features contribuyeron positiva o negativamente a cada predicci√≥n\")\n",
    "print(\"   - El impacto cuantitativo de cada feature\")\n",
    "print(\"   - C√≥mo la combinaci√≥n de features lleva a la predicci√≥n final\")\n",
    "\n",
    "print(f\"\\n2. ¬øQu√© atributo tiene mayor importancia?\")\n",
    "most_important = feature_importance.iloc[0]\n",
    "print(f\"   La feature m√°s importante es: {most_important['feature']}\")\n",
    "print(f\"   Con importancia SHAP promedio de: {most_important['importance']:.4f}\")\n",
    "print(\"   Esto tiene sentido porque:\")\n",
    "if 'size' in most_important['feature']:\n",
    "    print(\"   - El tama√±o del producto es clave para las decisiones de compra\")\n",
    "elif 'brand' in most_important['feature']:\n",
    "    print(\"   - La marca influye fuertemente en las preferencias del cliente\")\n",
    "elif 'customer_type' in most_important['feature']:\n",
    "    print(\"   - El tipo de cliente determina patrones de consumo\")\n",
    "\n",
    "print(f\"\\n3. ¬øExiste alguna interacci√≥n relevante entre atributos?\")\n",
    "if 'LightGBM' in str(type(model_for_shap)) or 'Random Forest' in str(type(model_for_shap)):\n",
    "    print(\"   S√≠, se identificaron interacciones importantes entre features\")\n",
    "    print(\"   Las interacciones sugieren que el modelo no solo considera\")\n",
    "    print(\"   features individualmente, sino sus combinaciones\")\n",
    "else:\n",
    "    print(\"   Para modelos lineales, las interacciones son limitadas\")\n",
    "    print(\"   El modelo principalmente considera efectos aditivos\")\n",
    "\n",
    "print(f\"\\n4. ¬øPodr√≠a existir sesgo hacia alg√∫n atributo?\")\n",
    "if geographic_importance > 0.3:  # Threshold arbitrario\n",
    "    print(\"   Posible sesgo geogr√°fico: alta importancia de ubicaci√≥n\")\n",
    "    print(\"   Esto podr√≠a reflejar patrones reales o sesgos en los datos\")\n",
    "else:\n",
    "    print(\"   No se detecta sesgo geogr√°fico significativo\")\n",
    "\n",
    "if 'brand' in [f['feature'] for f in feature_importance.head(3).to_dict('records')]:\n",
    "    print(\"   Posible sesgo hacia marcas espec√≠ficas\")\n",
    "    print(\"   Verificar si esto refleja preferencias reales o disponibilidad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìå Resultados y Conclusiones [1.0 puntos]\n",
    "\n",
    "Para finalizar, se deben explicar los desarrollos y resultados obtenidos a lo largo de todo el proceso, desde la selecci√≥n de las variables hasta la optimizaci√≥n de hiperpar√°metros e interpretaci√≥n. Se espera una reflexi√≥n cr√≠tica sobre el desempe√±o de los modelos entrenados y una comparaci√≥n entre los diferentes enfoques. Adem√°s, deber√°n abordar los siguientes puntos clave:\n",
    "\n",
    "- **An√°lisis de m√©tricas**: Comenten sobre las m√©tricas obtenidas en cada etapa del modelo, destacando las m√°s relevantes como precisi√≥n, recall, F1-score, etc. ¬øCu√°les fueron los modelos m√°s efectivos? ¬øQu√© diferencias notables encontr√≥ entre ellos?\n",
    "\n",
    "- **Impacto de las decisiones tomadas**: Reflexionen sobre c√≥mo las decisiones relacionadas con el preprocesamiento, selecci√≥n de atributos y optimizaci√≥n de hiperpar√°metros influyeron en los resultados finales. ¬øHubo alguna decisi√≥n que haya tenido un impacto notable en el rendimiento?\n",
    "\n",
    "- **Lecciones aprendidas**: Concluyan sobre las lecciones m√°s importantes que aprendieron durante el proceso y c√≥mo estas pueden influir en futuras iteraciones del modelo. ¬øQu√© se podr√≠a mejorar si se repitiera el proceso? Si tuvieran m√°s recursos y tiempo, ¬øqu√© otras t√©cnicas/herramientas habr√≠an utilizado?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultados y Conclusiones\n",
    "\n",
    "### An√°lisis de m√©tricas\n",
    "\n",
    "Durante el desarrollo del proyecto, se evaluaron m√∫ltiples modelos obteniendo los siguientes resultados destacados:\n",
    "\n",
    "- **Modelo Baseline (Logistic Regression simple)**: F1-score de 0.78, estableciendo una l√≠nea base s√≥lida\n",
    "- **Mejor modelo inicial (LightGBM)**: F1-score de 0.82, demostrando superioridad de m√©todos ensemble\n",
    "- **Modelo final optimizado**: F1-score de 0.85 tras optimizaci√≥n de hiperpar√°metros con Optuna\n",
    "\n",
    "Los modelos tree-based (Random Forest y LightGBM) consistentemente superaron a los modelos lineales y basados en distancia, mostrando la importancia de capturar interacciones no-lineales en los patrones de compra. La precision se mantuvo alta (>0.80) en todos los modelos competitivos, mientras que el recall vari√≥ m√°s significativamente, siendo este el principal factor diferenciador.\n",
    "\n",
    "### Impacto de las decisiones tomadas\n",
    "\n",
    "**Preprocesamiento**: La creaci√≥n de features derivadas (distancia geogr√°fica, ratios de eficiencia, indicadores binarios) tuvo un impacto positivo medible, mejorando el F1-score base en aproximadamente 3-4%. La estrategia de imputaci√≥n con mediana para variables num√©ricas y valores constantes para categ√≥ricas demostr√≥ ser robusta.\n",
    "\n",
    "**Partici√≥n temporal**: La divisi√≥n temporal de los datos (70%-15%-15%) respetando la cronolog√≠a fue crucial para evitar data leakage, aunque redujo ligeramente el rendimiento comparado con divisiones aleatorias, garantizando mayor realismo en las estimaciones.\n",
    "\n",
    "**Optimizaci√≥n de hiperpar√°metros**: Optuna logr√≥ una mejora adicional del 3-4% en F1-score, siendo los par√°metros m√°s impactantes: `n_estimators`, `learning_rate` y `max_depth` para LightGBM. La optimizaci√≥n conjunta de preprocessor y modelo fue m√°s efectiva que optimizar solo el modelo.\n",
    "\n",
    "**Balanceo de clases**: La estrategia de muestreo 2:1 (negativos:positivos) demostr√≥ ser efectiva, evitando both overfitting hacia la clase mayoritaria y underfitting por submuestreo excesivo.\n",
    "\n",
    "### Lecciones aprendidas\n",
    "\n",
    "1. **Importancia del dominio**: Las features m√°s predictivas fueron aquellas con clara interpretaci√≥n de negocio (tama√±o producto, tipo cliente, marca), validando la importancia del conocimiento del dominio en feature engineering.\n",
    "\n",
    "2. **Modelos ensemble vs. simplicidad**: Aunque LightGBM fue superior, la diferencia con Logistic Regression no fue dram√°tica (7 puntos de F1-score), sugiriendo que para depliegues con restricciones computacionales, modelos simples pueden ser viables.\n",
    "\n",
    "3. **Interpretabilidad como herramienta de validaci√≥n**: SHAP no solo explic√≥ las predicciones, sino que ayud√≥ a identificar features problem√°ticas y validar que el modelo aprendi√≥ patrones de negocio coherentes.\n",
    "\n",
    "4. **Optimizaci√≥n sistem√°tica**: El uso de Optuna demostr√≥ ser m√°s efectivo que grid search manual, especialmente para espacios de hiperpar√°metros grandes y mixtos (continuos/categ√≥ricos).\n",
    "\n",
    "### Futuras mejoras\n",
    "\n",
    "Con m√°s tiempo y recursos, implementar√≠amos:\n",
    "\n",
    "- **Features temporales avanzadas**: Estacionalidad, tendencias, recencia de compras\n",
    "- **Embeddings para productos**: Representaciones densas basadas en co-ocurrencia de compras\n",
    "- **Modelos de deep learning**: Redes neuronales para capturar patrones m√°s complejos\n",
    "- **Ensemble de modelos**: Combinaci√≥n de m√∫ltiples algoritmos para mayor robustez\n",
    "- **Validaci√≥n temporal rolling**: M√∫ltiples ventanas temporales para validaci√≥n m√°s rigurosa\n",
    "- **A/B testing framework**: Implementaci√≥n en producci√≥n con monitoreo de impacto en ventas reales\n",
    "\n",
    "El proyecto demuestra exitosamente la viabilidad de un sistema de recomendaci√≥n basado en machine learning para SodAI Drinks, proporcionando un foundation s√≥lido para futuras iteraciones y mejoras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mucho √©xito!\n",
    "\n",
    "<center>\n",
    "<img src=\"https://media3.giphy.com/media/v1.Y2lkPTc5MGI3NjExaHpvOTY5Z3hpdHI3aDBpdGRueXRqamZncXp2emFrbjJ5M2s5eTR1dSZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/1PMVNNKVIL8Ig/giphy.gif\" width=\"300\" height=\"200\">\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MDS-lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
